# 自然語言處理技術與應用 - 100題單選題題庫

基於 L21101_自然語言處理技術與應用.md

考試日期：2026/05/23

---

## 第一部分：核心概念與定義 (10題)

### 1. NLP (Natural Language Processing) 的主要目標是？
A. 處理圖像數據  
B. 讓電腦理解、處理和生成人類自然語言  
C. 管理數據庫  
D. 優化網路速度

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** NLP 是人工智慧的分支，旨在讓電腦能夠理解、處理和生成人類自然語言（文字或語音）。</span>

### 2. NLP 發展史中，哪個時期開始了深度學習革命？
A. 1950年代  
B. 1980-1990年代  
C. 2000年代  
D. 2010年代

<span style="color: white">**正確答案：D**</span>  
<span style="color: white">**解析：** 2010年代開始了深度學習革命，帶來了 NLP 技術的重大突破。</span>

### 3. Transformer 架構首次提出於哪一年？
A. 2012年  
B. 2014年  
C. 2017年  
D. 2020年

<span style="color: white">**正確答案：C**</span>  
<span style="color: white">**解析：** 2017年至今是 Transformer 架構與大型語言模型(LLM)時代。</span>

### 4. NLP 任務分類中，不屬於「理解任務」的是？
A. 文本分類  
B. 情感分析  
C. 命名實體識別  
D. 文本生成

<span style="color: white">**正確答案：D**</span>  
<span style="color: white">**解析：** 文本生成屬於「生成任務」，理解任務包括分類、情感分析、NER、語義分析等。</span>

### 5. NLP 的三大任務類型不包括？
A. 理解任務  
B. 生成任務  
C. 轉換任務  
D. 刪除任務

<span style="color: white">**正確答案：D**</span>  
<span style="color: white">**解析：** NLP 主要任務分為理解、生成、轉換三大類，沒有刪除任務這個分類。</span>

### 6. 下列哪個是 NLP 的轉換任務？
A. 文本分類  
B. 語音轉文字  
C. 情感分析  
D. 文本摘要

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 轉換任務包括語音轉文字、文字轉語音、語言轉換等。</span>

### 7. 1950年代 NLP 的主要研究方向是？
A. 深度學習  
B. 機器翻譯的早期嘗試  
C. Transformer  
D. 大型語言模型

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 1950年代主要是機器翻譯的早期嘗試階段。</span>

### 8. 2000年代 NLP 主要採用什麼方法？
A. 基於規則  
B. 統計機器學習  
C. 深度學習  
D. Transformer

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 2000年代主要使用統計機器學習方法。</span>

### 9. 下列哪個不是 NLP 的應用場景？
A. 機器翻譯  
B. 情感分析  
C. 圖像識別  
D. 對話系統

<span style="color: white">**正確答案：C**</span>  
<span style="color: white">**解析：** 圖像識別屬於電腦視覺領域，不是 NLP 的應用。</span>

### 10. LLM 代表什麼？
A. Large Language Model (大型語言模型)  
B. Long Learning Method  
C. Low Level Machine  
D. Latest Linear Model

<span style="color: white">**正確答案：A**</span>  
<span style="color: white">**解析：** LLM 是 Large Language Model 的縮寫，指大型語言模型。</span>

---

## 第二部分：文本預處理 (15題)

### 11. 中文分詞面臨的主要挑戰不包括？
A. 無明顯詞邊界  
B. 歧義問題  
C. 新詞識別  
D. 大小寫統一

<span style="color: white">**正確答案：D**</span>  
<span style="color: white">**解析：** 大小寫統一是英文處理的問題，中文沒有大小寫區分。</span>

### 12. 「南京市長江大橋」是中文分詞中什麼問題的經典例子？
A. 新詞識別  
B. 歧義問題  
C. 停用詞問題  
D. 詞形還原

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 可以切分為「南京市/長江大橋」或「南京/市長/江大橋」，是典型的歧義問題。</span>

### 13. 最大匹配法屬於哪種分詞方法？
A. 基於規則  
B. 基於統計  
C. 基於深度學習  
D. 基於知識圖譜

<span style="color: white">**正確答案：A**</span>  
<span style="color: white">**解析：** 最大匹配法是基於規則的分詞方法。</span>

### 14. BiLSTM-CRF 屬於哪種分詞方法？
A. 基於規則  
B. 基於統計  
C. 基於深度學習  
D. 基於詞典

<span style="color: white">**正確答案：C**</span>  
<span style="color: white">**解析：** BiLSTM-CRF 是基於深度學習的現代分詞方法。</span>

### 15. 停用詞移除的主要目的是？
A. 增加文本長度  
B. 移除對語義貢獻較小的常用詞  
C. 提高處理速度  
D. 增加詞彙量

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 停用詞移除是為了去除「的、了、在」等對語義貢獻較小的常用詞。</span>

### 16. 下列哪個是中文常見的停用詞？
A. running  
B. computer  
C. 的  
D. algorithm

<span style="color: white">**正確答案：C**</span>  
<span style="color: white">**解析：** 「的」是典型的中文停用詞。</span>

### 17. Stemming (詞幹提取) 的作用是？
A. 增加詞彙  
B. 將詞還原到詞根形式  
C. 刪除詞彙  
D. 翻譯詞彙

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 詞幹提取將詞還原到詞根形式，如 running→run。</span>

### 18. Lemmatization (詞形還原) 與 Stemming 的主要區別是？
A. 速度更快  
B. 考慮詞的語法角色，更準確  
C. 適用於中文  
D. 不需要詞典

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 詞形還原考慮詞的語法角色，還原到字典形式，比詞幹提取更準確。</span>

### 19. 中文為什麼較少使用 Stemming 和 Lemmatization？
A. 技術不成熟  
B. 中文沒有詞形變化  
C. 效果不好  
D. 計算量太大

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 中文沒有詞形變化，這些技術主要用於英文等有詞形變化的語言。</span>

### 20. 文本正規化處理不包括？
A. 大小寫統一  
B. 數字標準化  
C. 標點符號處理  
D. 圖像壓縮

<span style="color: white">**正確答案：D**</span>  
<span style="color: white">**解析：** 圖像壓縮不屬於文本正規化處理。</span>

### 21. Tokenization (分詞) 是什麼過程？
A. 刪除文本  
B. 將連續文本分割成詞彙單元  
C. 翻譯文本  
D. 壓縮文本

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** Tokenization 是將連續文本分割成單獨的詞彙單元的過程。</span>

### 22. HMM (隱馬爾可夫模型) 可用於？
A. 圖像處理  
B. 基於統計的分詞  
C. 數據壓縮  
D. 網路路由

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** HMM 是基於統計的分詞方法之一。</span>

### 23. N-gram 模型中的 "N" 代表什麼？
A. 詞的數量  
B. 連續詞的個數  
C. 文檔數量  
D. 用戶數量

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** N-gram 模型中 N 表示連續考慮的詞的個數，如 bigram (N=2)。</span>

### 24. 特殊字符清理屬於哪個預處理步驟？
A. 分詞  
B. 正規化處理  
C. 詞幹提取  
D. 特徵提取

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 特殊字符清理屬於正規化處理的一部分。</span>

### 25. 停用詞表應該如何使用？
A. 適用於所有任務  
B. 根據具體任務調整  
C. 越長越好  
D. 不需要使用

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 某些任務中停用詞可能包含重要信息，應根據具體任務調整。</span>

---

## 第三部分：特徵提取技術 (20題)

### 26. Bag of Words (BoW) 的主要缺點是？
A. 計算複雜  
B. 忽略詞序和語義關係  
C. 需要大量數據  
D. 無法處理中文

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** BoW 將文本表示為詞彙集合，忽略了詞的順序和語義關係。</span>

### 27. TF-IDF 中的 TF 代表什麼？
A. Total Frequency  
B. Term Frequency (詞頻)  
C. Text Format  
D. Transfer Function

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** TF 是 Term Frequency，表示詞在文檔中出現的頻率。</span>

### 28. TF-IDF 中的 IDF 主要衡量什麼？
A. 詞的常見程度  
B. 詞的稀有程度  
C. 詞的長度  
D. 詞的複雜度

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** IDF (Inverse Document Frequency) 衡量詞的稀有程度，稀有詞權重更高。</span>

### 29. TF-IDF 的計算公式是？
A. TF + IDF  
B. TF × IDF  
C. TF - IDF  
D. TF ÷ IDF

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** TF-IDF = TF × IDF</span>

### 30. Word2Vec 的兩種模型不包括？
A. Skip-gram  
B. CBOW  
C. Transformer  
D. 以上都包括

<span style="color: white">**正確答案：C**</span>  
<span style="color: white">**解析：** Word2Vec 包括 Skip-gram 和 CBOW 兩種模型，Transformer 是另一種架構。</span>

### 31. Skip-gram 模型的訓練目標是？
A. 用上下文預測中心詞  
B. 用中心詞預測上下文  
C. 翻譯文本  
D. 生成文本

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** Skip-gram 使用中心詞預測上下文詞彙。</span>

### 32. CBOW 模型的訓練目標是？
A. 用上下文預測中心詞  
B. 用中心詞預測上下文  
C. 詞性標註  
D. 句法分析

<span style="color: white">**正確答案：A**</span>  
<span style="color: white">**解析：** CBOW (Continuous Bag of Words) 使用上下文預測中心詞。</span>

### 33. Word2Vec 的優點不包括？
A. 捕捉語義相似性  
B. 低維稠密表示  
C. 考慮詞序  
D. 詞向量可計算

<span style="color: white">**正確答案：C**</span>  
<span style="color: white">**解析：** Word2Vec 不考慮詞的順序，這是其局限性之一。</span>

### 34. GloVe 與 Word2Vec 的主要區別是？
A. 速度更快  
B. 結合全局統計信息  
C. 只能處理英文  
D. 不需要訓練

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** GloVe (Global Vectors) 結合全局統計信息，提供更好的語義表示。</span>

### 35. FastText 相比 Word2Vec 的優勢是？
A. 訓練速度更快  
B. 考慮子詞(subword)信息  
C. 模型更小  
D. 不需要數據

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** FastText 考慮子詞信息，對未登錄詞(OOV)處理更好。</span>

### 36. 未登錄詞(OOV)是指？
A. 訓練時未見過的詞  
B. 停用詞  
C. 常見詞  
D. 標點符號

<span style="color: white">**正確答案：A**</span>  
<span style="color: white">**解析：** OOV (Out Of Vocabulary) 指訓練時未見過的詞彙。</span>

### 37. ELMo 的主要特點是？
A. 靜態詞向量  
B. 詞的表示依賴上下文  
C. 不需要訓練  
D. 只能處理英文

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** ELMo 使用雙向 LSTM，詞的表示依賴於具體上下文。</span>

### 38. 上下文嵌入(Conceptual Embeddings)與靜態詞向量的主要區別是？
A. 訓練時間  
B. 同一個詞在不同上下文中有不同表示  
C. 模型大小  
D. 語言支持

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 上下文嵌入中，同一個詞在不同上下文中會有不同的向量表示。</span>

### 39. BERT 使用的預訓練架構是？
A. 單向編碼器  
B. 雙向編碼器  
C. 解碼器  
D. 循環網路

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** BERT 使用雙向 Transformer 編碼器架構。</span>

### 40. BoW 主要應用於什麼任務？
A. 機器翻譯  
B. 文本分類、情感分析  
C. 語音識別  
D. 圖像處理

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** BoW 簡單直觀，主要用於文本分類和情感分析。</span>

### 41. TF-IDF 常用於什麼任務？
A. 圖像分類  
B. 文本檢索、關鍵詞提取  
C. 語音合成  
D. 視頻分析

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** TF-IDF 常用於文本檢索和關鍵詞提取任務。</span>

### 42. Word2Vec 訓練得到的詞向量具有什麼特性？
A. 語義相似的詞向量相近  
B. 所有詞向量相同  
C. 詞向量隨機分佈  
D. 詞向量維度固定為2

<span style="color: white">**正確答案：A**</span>  
<span style="color: white">**解析：** Word2Vec 能夠捕捉詞的語義相似性，相似詞的向量在空間中較近。</span>

### 43. 詞嵌入的典型維度是？
A. 10-50  
B. 50-300  
C. 1000-5000  
D. 10000以上

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 詞嵌入的典型維度通常在 50-300 之間，如 Word2Vec 常用 100-300維。</span>

### 44. 稠密表示(Dense Representation)與稀疏表示的主要區別是？
A. 維度大小  
B. 大部分元素為0 vs 大部分元素非0  
C. 訓練時間  
D. 準確率

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 稀疏表示（如 BoW）大部分元素為0，稠密表示（如詞嵌入）大部分元素非0。</span>

### 45. 下列哪個不是詞嵌入方法？
A. Word2Vec  
B. GloVe  
C. FastText  
D. SVM

<span style="color: white">**正確答案：D**</span>  
<span style="color: white">**解析：** SVM 是分類算法，不是詞嵌入方法。</span>

---

## 第四部分：深度學習模型 (25題)

### 46. RNN 的主要優點是？
A. 訓練速度快  
B. 能處理變長序列  
C. 不會梯度消失  
D. 參數少

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** RNN 使用循環連接，能夠處理變長序列數據。</span>

### 47. RNN 的主要缺點是？
A. 無法處理序列  
B. 梯度消失/爆炸問題  
C. 內存需求小  
D. 訓練速度快

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** RNN 存在梯度消失和梯度爆炸問題，難以學習長距離依賴。</span>

### 48. LSTM 的全稱是？
A. Long Short-Term Memory  
B. Large Scale Text Model  
C. Linear System Training Method  
D. Language Semantic Transfer Model

<span style="color: white">**正確答案：A**</span>  
<span style="color: white">**解析：** LSTM 是 Long Short-Term Memory 長短期記憶網路。</span>

### 49. LSTM 的核心機制不包括？
A. 遺忘門  
B. 輸入門  
C. 輸出門  
D. 編碼門

<span style="color: white">**正確答案：D**</span>  
<span style="color: white">**解析：** LSTM 包含遺忘門、輸入門、輸出門和細胞狀態，沒有編碼門。</span>

### 50. LSTM 相比 RNN 的主要改進是？
A. 速度更快  
B. 解決長期依賴問題  
C. 參數更少  
D. 不需要訓練

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** LSTM 通過門控機制解決了 RNN 的長期依賴問題。</span>

### 51. GRU 相比 LSTM 的特點是？
A. 參數更多  
B. 參數更少，訓練更快  
C. 效果更好  
D. 不能處理序列

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** GRU 是 LSTM 的簡化版本，參數更少，訓練更快。</span>

### 52. Transformer 架構的核心創新是？
A. 循環連接  
B. 自注意力機制  
C. 卷積操作  
D. 池化操作

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** Transformer 的核心是自注意力機制(Self-Attention)。</span>

### 53. Transformer 相比 RNN 的優勢不包括？
A. 並行計算效率高  
B. 捕捉長距離依賴  
C. 參數更少  
D. 可擴展性強

<span style="color: white">**正確答案：C**</span>  
<span style="color: white">**解析：** Transformer 通常參數更多，但並行效率高、可擴展性強。</span>

### 54. 位置編碼(Positional Encoding)在 Transformer 中的作用是？
A. 增加參數  
B. 提供序列位置信息  
C. 減少計算量  
D. 防止過擬合

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** Transformer 沒有循環結構，需要位置編碼提供詞的位置信息。</span>

### 55. 多頭注意力(Multi-Head Attention)的優勢是？
A. 速度更快  
B. 從不同子空間捕捉信息  
C. 參數更少  
D. 不需要訓練

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 多頭注意力允許模型從不同的表示子空間捕捉信息。</span>

### 56. Transformer 的架構包括？
A. 只有編碼器  
B. 只有解碼器  
C. 編碼器和解碼器  
D. 只有注意力層

<span style="color: white">**正確答案：C**</span>  
<span style="color: white">**解析：** 原始 Transformer 包含編碼器和解碼器兩部分。</span>

### 57. BERT-base 有多少參數？
A. 10M  
B. 110M  
C. 1B  
D. 10B

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** BERT-base 約有 110M (1.1億) 參數。</span>

### 58. BERT-large 有多少參數？
A. 110M  
B. 340M  
C. 1B  
D. 10B

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** BERT-large 約有 340M (3.4億) 參數。</span>

### 59. GPT-1 有多少參數？
A. 10M  
B. 117M  
C. 1.5B  
D. 175B

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** GPT-1 約有 117M 參數。</span>

### 60. GPT-3 有多少參數？
A. 117M  
B. 1.5B  
C. 175B  
D. 1T

<span style="color: white">**正確答案：C**</span>  
<span style="color: white">**解析：** GPT-3 約有 175B (1750億) 參數。</span>

### 61. BERT 的 MLM 任務是指？
A. 機器學習模型  
B. 掩碼語言模型  
C. 多語言模型  
D. 最大似然模型

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** MLM 是 Masked Language Model 掩碼語言模型。</span>

### 62. BERT 的 NSP 任務是指？
A. 自然語音處理  
B. 下一句預測  
C. 神經網路pruning  
D. 新詞預測

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** NSP 是 Next Sentence Prediction 下一句預測任務。</span>

### 63. RoBERTa 相比 BERT 的改進不包括？
A. 去除 NSP 任務  
B. 更大的批次  
C. 更長的訓練  
D. 完全不同的架構

<span style="color: white">**正確答案：D**</span>  
<span style="color: white">**解析：** RoBERTa 是優化的 BERT，架構基本相同，主要是訓練策略的改進。</span>

### 64. ALBERT 的主要創新是？
A. 參數共享  
B. 參數增加  
C. 速度提升  
D. 多語言支持

<span style="color: white">**正確答案：A**</span>  
<span style="color: white">**解析：** ALBERT 通過參數共享減少參數量，是參數共享的 BERT。</span>

### 65. T5 模型的特點是？
A. 只能分類  
B. 統一的文本到文本框架  
C. 只能翻譯  
D. 不需要訓練

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** T5 (Text-to-Text Transfer Transformer) 將所有 NLP 任務統一為文本到文本格式。</span>

### 66. GPT 系列是什麼類型的模型？
A. 雙向編碼器  
B. 自回歸生成模型  
C. 判別模型  
D. 聚類模型

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** GPT 是自回歸生成模型，逐個生成 token。</span>

### 67. 自回歸生成的特點是？
A. 同時生成所有詞  
B. 逐個生成 token  
C. 隨機生成  
D. 不生成新內容

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 自回歸生成是根據前文逐個生成下一個 token。</span>

### 68. Transformer 的前饋神經網路(FFN)在哪裡？
A. 只在編碼器  
B. 只在解碼器  
C. 編碼器和解碼器都有  
D. 不包含 FFN

<span style="color: white">**正確答案：C**</span>  
<span style="color: white">**解析：** Transformer 的編碼器和解碼器都包含前饋神經網路層。</span>

### 69. 預訓練模型的優勢不包括？
A. 利用大規模無標註數據  
B. 遷移學習能力  
C. 不需要任何數據  
D. 提升下游任務性能

<span style="color: white">**正確答案：C**</span>  
<span style="color: white">**解析：** 預訓練仍需要大規模數據，只是可以使用無標註數據。</span>

### 70. GPT-4 的主要特點是？
A. 只能處理文本  
B. 多模態能力  
C. 參數最少  
D. 速度最快

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** GPT-4 具備多模態能力，可以處理圖像和文本。</span>

---

## 第五部分：NLP 應用 (20題)

### 71. 文本分類的傳統方法不包括？
A. SVM  
B. 樸素貝葉斯  
C. Transformer  
D. 邏輯回歸

<span style="color: white">**正確答案：C**</span>  
<span style="color: white">**解析：** Transformer 是深度學習方法，不是傳統方法。</span>

### 72. 情感分析的任務類型不包括？
A. 二分類  
B. 多分類  
C. 細粒度分析  
D. 圖像分類

<span style="color: white">**正確答案：D**</span>  
<span style="color: white">**解析：** 圖像分類不是情感分析的任務類型。</span>

### 73. 命名實體識別(NER)的常見實體類型不包括？
A. 人名  
B. 地名  
C. 組織名  
D. 顏色

<span style="color: white">**正確答案：D**</span>  
<span style="color: white">**解析：** NER 主要識別人名、地名、組織名等專有名詞，顏色不是常見實體類型。</span>

### 74. BiLSTM-CRF 在 NER 中的優勢是？
A. 速度快  
B. 結合雙向上下文和序列標註  
C. 參數少  
D. 不需要訓練

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** BiLSTM-CRF 結合雙向 LSTM 捕捉上下文，CRF 進行序列標註。</span>

### 75. CRF 在 NER 中的作用是？
A. 特徵提取  
B. 考慮標籤間的轉移關係  
C. 數據清理  
D. 模型壓縮

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** CRF (條件隨機場) 考慮標籤間的轉移關係，如 B-PER 後不能直接跟 I-LOC。</span>

### 76. 神經機器翻譯(NMT)的代表架構是？
A. 決策樹  
B. Seq2Seq  
C. K-means  
D. SVM

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** Seq2Seq (序列到序列) 模型是 NMT 的經典架構。</span>

### 77. Seq2Seq 模型包括哪兩部分？
A. 編碼器和解碼器  
B. 輸入和輸出  
C. 前端和後端  
D. 訓練和測試

<span style="color: white">**正確答案：A**</span>  
<span style="color: white">**解析：** Seq2Seq 模型包含編碼器(Encoder)和解碼器(Decoder)。</span>

### 78. 機器翻譯的發展順序是？
A. NMT → SMT → 規則  
B. 規則 → SMT → NMT  
C. SMT → 規則 → NMT  
D. 只有 NMT

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 機器翻譯發展經歷了基於規則 → 統計機器翻譯(SMT) → 神經機器翻譯(NMT)。</span>

### 79. 問答系統的類型不包括？
A. 檢索式問答  
B. 生成式問答  
C. 閱讀理解  
D. 圖像編輯

<span style="color: white">**正確答案：D**</span>  
<span style="color: white">**解析：** 圖像編輯不是問答系統的類型。</span>

### 80. RAG 在問答系統中的作用是？
A. 壓縮模型  
B. 檢索增強生成  
C. 加速訓練  
D. 數據清理

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** RAG (Retrieval-Augmented Generation) 通過檢索相關信息增強生成質量。</span>

### 81. 抽取式摘要的方法是？
A. 生成新句子  
B. 從原文中提取句子  
C. 翻譯文本  
D. 刪除文本

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 抽取式摘要從原文中選擇關鍵句子組成摘要。</span>

### 82. 生成式摘要的方法是？
A. 從原文提取  
B. 生成新的摘要文本  
C. 複製原文  
D. 刪除句子

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 生成式摘要會生成新的摘要文本，可能包含原文沒有的表述。</span>

### 83. TextRank 是什麼類型的摘要方法？
A. 生成式  
B. 抽取式  
C. 翻譯式  
D. 混合式

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** TextRank 是基於圖的抽取式摘要方法。</span>

### 84. 對話系統的類型不包括？
A. 任務導向型  
B. 開放域對話  
C. 混合式  
D. 數據庫管理

<span style="color: white">**正確答案：D**</span>  
<span style="color: white">**解析：** 數據庫管理不是對話系統的類型。</span>

### 85. 檢索式對話系統的特點是？
A. 生成新回復  
B. 從預定義回復中選擇  
C. 翻譯對話  
D. 刪除對話

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 檢索式對話從預定義的回復庫中選擇最合適的回復。</span>

### 86. 生成式對話系統的特點是？
A. 從庫中選擇  
B. 生成新的回復  
C. 不回復  
D. 只能說固定句子

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 生成式對話系統能夠生成新的、靈活的回復。</span>

### 87. 垃圾郵件檢測屬於什麼 NLP 任務？
A. 機器翻譯  
B. 文本分類  
C. 摘要生成  
D. 對話系統

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 垃圾郵件檢測是二分類問題，屬於文本分類任務。</span>

### 88. 產品評論分析主要使用什麼 NLP 技術？
A. 機器翻譯  
B. 情感分析  
C. 語音識別  
D. 圖像處理

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 產品評論分析主要使用情感分析技術判斷正面或負面評價。</span>

### 89. 智能客服系統需要哪些 NLP 技術？
A. 意圖識別  
B. 實體識別  
C. 對話管理  
D. 以上皆是

<span style="color: white">**正確答案：D**</span>  
<span style="color: white">**解析：** 智能客服需要意圖識別、實體識別、對話管理等多種技術。</span>

### 90. 閱讀理解任務的目標是？
A. 翻譯文章  
B. 從給定文本中提取答案  
C. 生成新文章  
D. 刪除文本

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 閱讀理解任務是根據問題從給定文本中找到或提取答案。</span>

---

## 第六部分：評估指標 (6題)

### 91. BLEU 主要用於評估什麼任務？
A. 文本分類  
B. 機器翻譯  
C. 情感分析  
D. 命名實體識別

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** BLEU 是機器翻譯質量的主要評估指標。</span>

### 92. ROUGE 主要用於評估什麼任務？
A. 機器翻譯  
B. 摘要生成  
C. 文本分類  
D. 對話系統

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** ROUGE 主要用於評估摘要生成質量。</span>

### 93. BERTScore 的特點是？
A. 基於詞彙重疊  
B. 基於語義相似度  
C. 速度最快  
D. 不需要參考

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** BERTScore 基於 BERT 的語義相似度評估，考慮語義而非僅詞彙匹配。</span>

### 94. 困惑度(Perplexity)用於評估什麼？
A. 分類準確率  
B. 語言模型質量  
C. 翻譯質量  
D. 情感極性

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 困惑度用於評估語言模型的質量，值越低越好。</span>

### 95. F1-Score 適用於什麼任務？
A. 只能用於翻譯  
B. 分類任務  
C. 只能用於摘要  
D. 不適用於 NLP

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** F1-Score 是精確率和召回率的調和平均，適用於分類任務。</span>

### 96. 混淆矩陣主要用於？
A. 語言建模  
B. 可視化分類結果  
C. 文本生成  
D. 數據清理

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 混淆矩陣用於可視化分類結果，展示 TP、TN、FP、FN。</span>

---

## 第七部分：技術挑戰 (4題)

### 97. NLP 面臨的歧義問題不包括？
A. 一詞多義  
B. 語法歧義  
C. 語音歧義  
D. 圖像歧義

<span style="color: white">**正確答案：D**</span>  
<span style="color: white">**解析：** 圖像歧義不是 NLP 的問題，而是電腦視覺的問題。</span>

### 98. 低資源語言面臨的主要挑戰是？
A. 計算資源不足  
B. 訓練數據不足  
C. 模型太複雜  
D. 評估困難

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 低資源語言的主要挑戰是標註訓練數據稀缺。</span>

### 99. 多任務學習的優勢是？
A. 訓練速度更快  
B. 同時學習多個相關任務，提升泛化  
C. 模型更小  
D. 不需要數據

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 多任務學習通過同時學習多個相關任務，提升模型的泛化能力。</span>

### 100. 數據增強在 NLP 中的作用是？
A. 減少數據  
B. 生成更多訓練數據  
C. 刪除數據  
D. 壓縮數據

<span style="color: white">**正確答案：B**</span>  
<span style="color: white">**解析：** 數據增強通過各種技術生成更多訓練樣本，提升模型性能。</span>

---

## 答案速查表

| 題號 | 答案 | 題號 | 答案 | 題號 | 答案 | 題號 | 答案 |
|-----|------|-----|------|-----|------|-----|------|
| 1 | B | 26 | B | 51 | B | 76 | B |
| 2 | D | 27 | B | 52 | B | 77 | A |
| 3 | C | 28 | B | 53 | C | 78 | B |
| 4 | D | 29 | B | 54 | B | 79 | D |
| 5 | D | 30 | C | 55 | B | 80 | B |
| 6 | B | 31 | B | 56 | C | 81 | B |
| 7 | B | 32 | A | 57 | B | 82 | B |
| 8 | B | 33 | C | 58 | B | 83 | B |
| 9 | C | 34 | B | 59 | B | 84 | D |
| 10 | A | 35 | B | 60 | C | 85 | B |
| 11 | D | 36 | A | 61 | B | 86 | B |
| 12 | B | 37 | B | 62 | B | 87 | B |
| 13 | A | 38 | B | 63 | D | 88 | B |
| 14 | C | 39 | B | 64 | A | 89 | D |
| 15 | B | 40 | B | 65 | B | 90 | B |
| 16 | C | 41 | B | 66 | B | 91 | B |
| 17 | B | 42 | A | 67 | B | 92 | B |
| 18 | B | 43 | B | 68 | C | 93 | B |
| 19 | B | 44 | B | 69 | C | 94 | B |
| 20 | D | 45 | D | 70 | B | 95 | B |
| 21 | B | 46 | B | 71 | C | 96 | B |
| 22 | B | 47 | B | 72 | D | 97 | D |
| 23 | B | 48 | A | 73 | D | 98 | B |
| 24 | B | 49 | D | 74 | B | 99 | B |
| 25 | B | 50 | B | 75 | B | 100 | B |

---

**題目分佈統計**

| 主題 | 題數 | 題號範圍 |
|------|------|---------|
| 核心概念與定義 | 10 | 1-10 |
| 文本預處理 | 15 | 11-25 |
| 特徵提取技術 | 20 | 26-45 |
| 深度學習模型 | 25 | 46-70 |
| NLP應用 | 20 | 71-90 |
| 評估指標 | 6 | 91-96 |
| 技術挑戰 | 4 | 97-100 |
| **總計** | **100** | |

---

**考試日期：2026/05/23**

**備註：**
- 所有題目基於 L21101_自然語言處理技術與應用.md
- 涵蓋從基礎概念到深度學習模型的完整知識體系
- 建議結合原文檔深入學習
