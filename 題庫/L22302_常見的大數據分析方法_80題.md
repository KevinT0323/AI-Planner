# 常見的大數據分析方法 - 80題單選題題庫

基於 L22302_常見的大數據分析方法.md

考試日期：2026/05/23

---

## 第一部分：機器學習基礎 - 監督學習 (20題)

### 1. 監督學習 (Supervised Learning) 的主要特徵是？
A. 資料沒有標籤 (Label)  
B. 資料擁有對應的標籤或目標值  
C. 用於探索資料的內在結構  
D. 不需要訓練

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 監督學習使用標記好的數據（輸入-輸出對）來訓練模型，以預測未來的結果。

</details>

### 2. 下列哪種問題適合使用「線性回歸 (Linear Regression)」？
A. 垃圾郵件分類  
B. 房價預測 (連續數值)  
C. 圖像識別  
D. 語音生成

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 線性回歸用於預測連續的數值型變數，如價格、氣溫。

</details>

### 3. 「邏輯回歸 (Logistic Regression)」雖然名字有回歸，但它主要用於？
A. 聚類問題  
B. 異常檢測  
C. 分類問題 (如二元分類)  
D. 降維

<details>
<summary>顯示答案</summary>

**正確答案：C**  
**解析：** 邏輯回歸透過 Sigmoid 函數輸出 0 到 1 的機率，常用於分類（如是/否）。

</details>

### 4. 決策樹 (Decision Tree) 的優點之一是？
A. 黑箱模型不可解釋  
B. 需要大量的數據預處理  
C. 規則清晰，可解釋性強  
D. 只能處理二分類

<details>
<summary>顯示答案</summary>

**正確答案：C**  
**解析：** 決策樹生成類似流程圖的規則，易於人類理解和解釋。

</details>

### 5. 隨機森林 (Random Forest) 屬於哪種類型的機器學習方法？
A. 單一模型  
B. 集成學習 (Ensemble Learning) 之 Bagging  
C. 無監督學習  
D. 強化學習

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 隨機森林集成多棵決策樹，通過投票或平均來提高預測穩健性。

</details>

### 6. 梯度提升樹 (GBDT, Gradient Boosting) 的核心思想是？
A. 並行訓練多個模型  
B. 每個新模型都在修正前一個模型的錯誤 (殘差)  
C. 隨機刪除數據  
D. 只使用一棵樹

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** Boosting 是一種迭代算法，每一步都試圖減少前一步的預測誤差。

</details>

### 7. 支援向量機 (SVM) 試圖找到什麼？
A. 數據的中心點  
B. 區分不同類別的最佳超平面 (Hyperplane)，使間隔最大化  
C. 最短路徑  
D. 數據的密度

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** SVM 通過最大化分類邊界（Margin）來提高模型的泛化能力。

</details>

### 8. 在分類問題中，XGBoost 通常被認為是？
A. 性能很差的演算法  
B. 一種高效、可擴展的梯度提升實現，常在競賽中獲勝  
C. 只能用於回歸  
D. 深度學習模型

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** XGBoost 對 GBDT 進行了優化，速度快且效果好，是大數據競賽的神器。

</details>

### 9. 神經網路 (Neural Networks) 的基本單元是？
A. 決策樹  
B. 神經元 (Neuron) / 感知機  
C. 支援向量  
D. 簇

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 人工神經網路由模擬生物神經元的人工神經元連接而成。

</details>

### 10. 深度學習 (Deep Learning) 與傳統神經網路的主要區別在於？
A. 沒有區別  
B. 深度學習擁有更多隱藏層 (Deep)，能自動提取特徵  
C. 深度學習只能處理文本  
D. 深度學習不需要數據

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** "深度" 指的是神經網路的層數多，使其能學習複雜的抽象特徵。

</details>

### 11. KNN (K-Nearest Neighbors) 演算法的預測依據是？
A. 線性方程  
B. 最近的 K 個鄰居的類別或數值  
C. 全局平均值  
D. 決策樹規則

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** KNN 是一種基於實例的學習（懶惰學習），根據鄰居投票或平均。

</details>

### 12. 關於過擬合 (Overfitting)，下列描述正確的是？
A. 模型在訓練集和測試集上表現都很好  
B. 模型在訓練集上表現極好，但在新數據 (測試集) 上表現很差  
C. 模型太簡單，抓不住數據特徵  
D. 模型訓練時間太短

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 過擬合指模型死記硬背了訓練數據（包含噪聲），導致泛化能力差。

</details>

### 13. 為了解決過擬合，隨機森林使用了什麼策略？
A. 特徵隨機選取與樣本隨機採樣 (Bootstrap)  
B. 增加樹的深度  
C. 減少數據量  
D. 增加樹的數量無限多

<details>
<summary>顯示答案</summary>

**正確答案：A**  
**解析：** 隨機性降低了每棵樹的相關性，從而降低了整體模型的變異數。

</details>

### 14. 樸素貝葉斯 (Naive Bayes) 分類器基於什麼假設？
A. 所有特徵之間相互獨立  
B. 所有特徵高度相關  
C. 數據服從均勻分佈  
D. 樣本均衡

<details>
<summary>顯示答案</summary>

**正確答案：A**  
**解析：** "樸素" 指的是假設特徵條件獨立，這雖然簡單但往往有效。

</details>

### 15. 在回歸問題中，常用的評估指標是？
A. 準確率 (Accuracy)  
B. 均方誤差 (MSE) 或 均方根誤差 (RMSE)  
C. 精確率 (Precision)  
D. 召回率 (Recall)

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 對於連續數值預測，衡量預測值與真實值的差距通常用 MSE/RMSE。

</details>

### 16. 下列哪個演算法不需要對數據進行特徵縮放 (Feature Scaling)？
A. SVM  
B. 邏輯回歸  
C. 決策樹 / 隨機森林  
D. K-means

<details>
<summary>顯示答案</summary>

**正確答案：C**  
**解析：** 樹模型基於規則分割，不依賴距離度量，因此對特徵的數值範圍不敏感。

</details>

### 17. 交叉驗證 (Cross-Validation) 的主要目的是？
A. 增加訓練數據  
B. 更可靠地評估模型性能，避免單次劃分的偶然性  
C. 加速訓練  
D. 減少特徵

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 如 K-fold 交叉驗證，輪流驗證，充分利用數據評估泛化能力。

</details>

### 18. 正則化 (Regularization) 如 L1, L2 的作用是？
A. 讓係數變大  
B. 增加模型複雜度  
C. 懲罰過大的權重，防止過擬合  
D. 消除偏差

<details>
<summary>顯示答案</summary>

**正確答案：C**  
**解析：** 通過在損失函數中加入懲罰項，限制模型複雜度。

</details>

### 19. 在信用卡詐欺檢測中，數據通常極度不平衡，這時應關注哪個指標？
A. 總體準確率 (Accuracy)  
B. 召回率 (Recall) 與 Precision-Recall 曲線  
C. 訓練時間  
D. MSE

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 99% 的準確率可能意味著完全沒抓到詐欺樣本；關注 Recall 能確保抓出壞人。

</details>

### 20. SVM 中的 "核函數 (Kernel Trick)" 用於？
A. 將數據映射到高維空間以解決非線性分類問題  
B. 壓縮數據  
C. 降低維度  
D. 線性回歸

<details>
<summary>顯示答案</summary>

**正確答案：A**  
**解析：** 核函數允許 SVM 在高維空間尋找超平面，而無需顯式計算高維坐標，解決非線性問題。

</details>

---

## 第二部分：機器學習基礎 - 無監督學習 (15題)

### 21. K-means 聚類算法的 "K" 代表？
A. 迭代次數  
B. 預先指定的簇 (Cluster) 數量  
C. 距離閾值  
D. 數據維度

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** K-means 需要用戶指定將數據分成 K 類。

</details>

### 22. K-means 的核心步驟是？
A. 計算梯度  
B. 迭代更新簇中心 (Centroids) 和重新分配樣本  
C. 構建樹結構  
D. 矩陣分解

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 分配樣本到最近中心 -> 更新中心 -> 重複直到收斂。

</details>

### 23. 階層式聚類 (Hierarchical Clustering) 的結果通常顯示為？
A. 散點圖  
B. 樹狀圖 (Dendrogram)  
C. 柱狀圖  
D. 圓餅圖

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 樹狀圖展示了聚類的層次結構和合併過程。

</details>

### 24. DBSCAN 算法相較於 K-means 的優勢在於？
A. 必須預先指定 K  
B. 能發現任意形狀的簇，且能識別噪聲 (Outliers)  
C. 速度最快  
D. 只能發現球形簇

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 基於密度的聚類 (DBSCAN) 不需要指定 K，且對非凸形狀和噪聲有很好的魯棒性。

</details>

### 25. 關聯規則挖掘 (Association Rule Mining) 的經典案例是？
A. 垃圾郵件過濾  
B. 啤酒與尿布 (購物籃分析)  
C. 手寫數字識別  
D. 房價預測

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 發現商品之間的共現關係。

</details>

### 26. Apriori 演算法用於？
A. 挖掘頻繁項集 (Frequent Itemsets)  
B. 聚類  
C. 分類  
D. 回歸

<details>
<summary>顯示答案</summary>

**正確答案：A**  
**解析：** Apriori 是挖掘關聯規則的經典算法，利用頻繁項集的子集必頻繁的性質剪枝。

</details>

### 27. 在關聯規則中，"支持度 (Support)" 為 0.1 意味著？
A. 規則的可信度為 10%  
B. 該商品組合在所有交易中出現的比例為 10%  
C. 該規則提升了 10% 的銷售  
D. 10% 的用戶喜歡它

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 支持度表示項集在數據集中出現的頻率。

</details>

### 28. "置信度 (Confidence)" P(B|A) 衡量的是？
A. 商品 A 和 B 同時出現的機率  
B. 購買 A 的情況下，同時購買 B 的條件機率  
C. 規則的覆蓋範圍  
D. B 出現的機率

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 置信度代表規則的可靠性或強度。

</details>

### 29. 異常檢測 (Anomaly Detection) 在無監督學習中常基於什麼假設？
A. 異常點的特徵與大多數正常數據顯著不同 (如密度低、距離遠)  
B. 異常點很常見  
C. 所有數據都是異常的  
D. 異常點有標籤

<details>
<summary>顯示答案</summary>

**正確答案：A**  
**解析：** 孤立森林、DBSCAN 等都利用異常點的疏離性進行檢測。

</details>

### 30. 主成分分析 (PCA) 用於無監督學習中的？
A. 聚類  
B. 降維 (Dimensionality Reduction)  
C. 關聯分析  
D. 分類

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** PCA 將高維數據投影到低維空間，保留最大變異。

</details>

### 31. t-SNE 主要用於？
A. 線性降維  
B. 高維數據的可視化 (保留局部結構)  
C. 預測數值  
D. 壓縮圖片

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** t-SNE 擅長將高維數據映射到 2D 或 3D 進行觀察。

</details>

### 32. 如何選擇 K-means 的最佳 K 值？
A. 隨機選擇  
B. 使用手肘法 (Elbow Method) 或 輪廓係數 (Silhouette Score)  
C. 越大越好  
D. 總是設為 3

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 這些方法幫助在簇內緊密度和簇間分離度之間找到平衡。

</details>

### 33. 無監督學習的一大挑戰是？
A. 數據太多  
B. 缺乏標籤 (Ground Truth)，難以客觀評估結果好壞  
C. 算法太少  
D. 計算太快

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 沒有正確答案對照，結果的解釋往往依賴領域知識。

</details>

### 34. 購物籃分析屬於？
A. 分類問題  
B. 關聯分析  
C. 時間序列分析  
D. 聚類分析

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 分析商品間的關聯性。

</details>

### 35. 孤立森林 (Isolation Forest) 檢測異常的原理是？
A. 異常點更難被隔離  
B. 異常點更容易被隔離 (路徑更短)  
C. 計算密度  
D. 計算距離中心點距離

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 異常點通常「疏離」，在隨機樹中很容易被切分出來（路徑短）。

</details>

---

## 第三部分：深度學習與進階技術 (20題)

### 36. 卷積神經網路 (CNN) 最擅長處理的數據類型是？
A. 表格數據  
B. 圖像與視頻 (網格結構數據)  
C. 時間序列  
D. 文本

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** CNN 的卷積核能有效提取局部的空間特徵，是電腦視覺的首選。

</details>

### 37. 循環神經網路 (RNN) 最擅長處理的數據類型是？
A. 靜態圖像  
B. 序列數據 (如文本、語音、時間序列)  
C. 隨機噪聲  
D. 表格

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** RNN 具有記憶功能，能處理前後文相關的序列資訊。

</details>

### 38. 在深度學習中，"Epoch" 代表？
A. 一次迭代  
B. 整個訓練數據集被模型訓練過一次  
C. 一個批次 (Batch)  
D. 訓練結束

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 1 Epoch = 所有訓練樣本都過了一遍模型。

</details>

### 39. LSTM (長短期記憶網路) 是為了解決傳RNN的什麼問題？
A. 計算太慢  
B. 梯度消失/梯度爆炸，導致無法學習長序列依賴  
C. 參數太多  
D. 只能處理圖像

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** LSTM 引入門控機制，能有效捕捉長距離的依賴關係。

</details>

### 40. 自動編碼器 (Autoencoder) 是一種？
A. 監督學習模型  
B. 無監督學習神經網路，用於特徵學習或降維  
C. 決策樹  
D. 分類器

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 輸入 -> 壓縮 (編碼) -> 重構 (解碼) -> 輸出，學習數據的壓縮表示。

</details>

### 41. 深度學習中常用的激活函數 ReLU 是？
A. Sigmoid  
B. Tanh  
C. Rectified Linear Unit (修正線性單元)  
D. Softmax

<details>
<summary>顯示答案</summary>

**正確答案：C**  
**解析：** f(x) = max(0, x)，解決了深層網絡梯度消失問題，計算簡單。

</details>

### 42. Softmax 函數通常用於神經網路的哪一層？
A. 輸入層  
B. 卷積層  
C. 多分類問題的輸出層  
D. 隱藏層

<details>
<summary>顯示答案</summary>

**正確答案：C**  
**解析：** 將輸出轉換為機率分佈，總和為 1。

</details>

### 43. 遷移學習 (Transfer Learning) 是指？
A. 將一個模型從一台電腦移到另一台  
B. 利用在一個任務 (源域) 上訓練好的模型知識，應用到另一個相關任務 (目標域)  
C. 重新訓練模型  
D. 數據遷移

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 站在巨人的肩膀上，常用於小樣本數據訓練（使用預訓練模型微調）。

</details>

### 44. GAN (生成對抗網路) 由哪兩部分組成？
A. Encoder 和 Decoder  
B. Generator (生成器) 和 Discriminator (判別器)  
C. Map 和 Reduce  
D. Tree 和 Forest

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 生成器造假，判別器抓假，兩者博弈共同進步。

</details>

### 45. Transformer 架構 (如 BERT, GPT) 的核心機制是？
A. 卷積  
B. 循環  
C. 自注意力機制 (Self-Attention)  
D. 遞歸

<details>
<summary>顯示答案</summary>

**正確答案：C**  
**解析：** Attention 機制讓模型能並行處理序列並關注上下文的重要部分，引領了 NLP 的革命。

</details>

### 46. 深度學習在電腦視覺中的經典應用不包括？
A. 物件偵測 (Object Detection)  
B. 圖像分割 (Segmentation)  
C. 人臉識別  
D. 股票高頻交易 (主要用時間序列/RL)

<details>
<summary>顯示答案</summary>

**正確答案：D**  
**解析：** 雖然可以用 DL，但不是視覺領域的應用。

</details>

### 47. Dropout 是一種什麼技術？
A. 加速訓練  
B. 正則化技術，訓練時隨機「丟棄」部分神經元以防止過擬合  
C. 刪除數據  
D. 優化器

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 減少神經元之間的依賴，增強模型魯棒性。

</details>

### 48. SGD (隨機梯度下降) 是一種？
A. 損失函數  
B. 優化算法 (Optimizer)，用於更新權重  
C. 評估指標  
D. 網絡結構

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 用於最小化損失函數。

</details>

### 49. BERT 模型的主要創新是？
A. 單向語言模型  
B. 雙向 Transformer 編碼器，學習上下文語境  
C. 只能生成文本  
D. 用於圖像

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 雙向 (Bidirectional) 理解能力使其在理解任務上表現卓越。

</details>

### 50. 大語言模型 (LLM) 最典型的特徵是？
A. 參數規模極大，具備湧現能力 (Emergent Abilities)  
B. 只能做加法  
C. 完全依賴規則  
D. 不需要訓練

<details>
<summary>顯示答案</summary>

**正確答案：A**  
**解析：** 如 GPT-4，規模帶來質變。

</details>

---

## 第四部分：時間序列與分析框架 (25題)

### 51. ARIMA 模型適用於？
A. 圖像分類  
B. 平穩的時間序列預測  
C. 文本生成  
D. 聚類

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 自回歸整合移動平均模型是經典的時間序列預測方法。

</details>

### 52. 時間序列的 "季節性 (Seasonality)" 指的是？
A. 長期趨勢  
B. 隨機波動  
C. 固定週期重複出現的模式 (如每年聖誕節)  
D. 異常值

<details>
<summary>顯示答案</summary>

**正確答案：C**  
**解析：** 週期性的規律波動。

</details>

### 53. 時間序列分解通常將序列分解為哪三個部分？
A. 過去、現在、未來  
B. 趨勢 (Trend)、季節性 (Seasonality)、殘差/隨機項 (Residual/Noise)  
C. 輸入、處理、輸出  
D. 訓練、驗證、測試

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 經典分解模型：Y = T + S + R。

</details>

### 54. 處理大數據機器學習時，"數據並行化 (Data Parallelism)" 是指？
A. 切分模型到不同節點  
B. 將數據切分到不同節點，每個節點跑完整的模型副本，最後通過參數伺服器同步梯度  
C. 串行處理  
D. 減少數據

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 最常用的分散式訓練策略。

</details>

### 55. "模型並行化 (Model Parallelism)" 適用於？
A. 數據量極大  
B. 模型參數極大，單個 GPU/節點的內存放不下整個模型  
C. 小模型  
D. 簡單運算

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 將模型的不同層或部分拆分到不同設備上。

</details>

### 56. 增量學習 (Incremental Learning) 的特點是？
A. 每次都重練所有數據  
B. 模型能不斷從新數據中學習與更新，而無需完全重新訓練  
C. 學習速度慢  
D. 只能離線進行

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 適應數據流的變化，節省重練成本。

</details>

### 57. 線上學習 (Online Learning) 通常用於？
A. 歷史數據分析  
B. 實時數據流環境，模型即時更新 (如廣告點擊預測)  
C. 靜態報告  
D. 離線挖掘

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 模型隨數據一一到達而即時更新。

</details>

### 58. 分散式機器學習框架 Spark MLlib 支持？
A. 深度學習訓練 (原生支持較弱，通常對接 TF/PyTorch)  
B. 常見的傳統機器學習算法 (分類、回歸、聚類、推薦) 的分散式實現  
C. 單機運算  
D. Excel 宏

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** Spark MLlib 專注於傳統 ML 的並行化。

</details>

### 59. 參數伺服器 (Parameter Server) 架構在大規模深度學習中的作用是？
A. 存儲和同步全局模型參數  
B. 存儲訓練數據  
C. 執行前向傳播  
D. 顯示圖表

<details>
<summary>顯示答案</summary>

**正確答案：A**  
**解析：** Worker 計算梯度，Parameter Server 匯總梯度並更新權重。

</details>

### 60. ARIMA 模型中的 "I" (Integration) 代表？
A. 積分  
B. 差分 (Differencing)，用於將非平穩序列轉化為平穩序列  
C. 智慧  
D. 迭代

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 通過差分消除趨勢。

</details>

### 61. 移動平均 (Moving Average) 常用於？
A. 預測未來  
B. 平滑時間序列，消除短期波動，觀察趨勢  
C. 增加噪聲  
D. 分類

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 簡單有效的趨勢分析工具。

</details>

### 62. 在評估分類模型時，"F1-Score" 是？
A. 準確率  
B. 精確率 (Precision) 和 召回率 (Recall) 的調和平均數  
C. 錯誤率  
D. 運算速度

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 綜合考量 Precision 和 Recall，適合類別不平衡問題。

</details>

### 63. 自相關 (Autocorrelation) 是指？
A. 兩個不同變數的相關性  
B. 時間序列與其自身滯後 (Lag) 版本之間的相關性  
C. 沒有相關性  
D. 線性關係

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 衡量過去的值對未來值的影響。

</details>

### 64. Federation Learning (聯邦學習) 是一種？
A. 集中式學習  
B. 分散式隱私保護機器學習，數據不出本地，只交換加密梯度  
C. 無監督學習  
D. 雲端學習

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** "數據不動模型動"，保護隱私。

</details>

### 65. 超參數調優 (Hyperparameter Tuning) 的方法不包括？
A. 網格搜索 (Grid Search)  
B. 隨機搜索 (Random Search)  
C. 貝葉斯優化  
D. 梯度下降 (這是更新模型參數，不是超參數)

<details>
<summary>顯示答案</summary>

**正確答案：D**  
**解析：** 梯度下降用於訓練權重 Parameter，而非 Hyperparameter。

</details>

### 66. 關於時間序列預測，Prophet 模型是由哪家公司開源的？
A. Google  
B. Microsoft  
C. Facebook (Meta)  
D. Apple

<details>
<summary>顯示答案</summary>

**正確答案：C**  
**解析：** Prophet 擅長處理具有強烈季節性和缺失值的商業時間序列。

</details>

### 67. 在大數據框架下，All-Reduce 算法用於？
A. 數據壓縮  
B. 高效的梯度聚合與同步 (替代中心化的參數伺服器)  
C. 數據刪除  
D. 降維

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 環狀 All-Reduce 讓多 GPU 訓練通信效率最大化。

</details>

### 68. 什麼是 "平穩 (Stationary)" 時間序列？
A. 數值永遠不變  
B. 統計特性 (均值、變異數) 不隨時間改變  
C. 波動很大  
D. 有明顯趨勢

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 許多時間序列模型 (如 ARIMA) 假設序列是平穩的。

</details>

### 69. ROC 曲線下方的面積稱為？
A. ACC  
B. AUC (Area Under Curve)  
C. MSE  
D. F1

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** AUC 值越大，分類器效果越好。

</details>

### 70. 關聯規則中的 "Lift" (提升度) > 1 代表？
A. A 和 B 獨立  
B. A 的出現對 B 的出現有正向促進作用  
C. A 的出現抑制 B  
D. 規則無效

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** Lift > 1 表示正相關，Lift = 1 獨立，Lift < 1 負相關。

</details>

### 71. 在模型評估中，將正樣本預測為負樣本稱為？
A. TP  
B. TN  
C. FP  
D. FN (False Negative)

<details>
<summary>顯示答案</summary>

**正確答案：D**  
**解析：** 漏報 (False Negative)。

</details>

### 72. 哪種聚類方法不需要預先指定簇的數量？
A. K-means  
B. DBSCAN  
C. 決策樹  
D. 邏輯回歸

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** DBSCAN 自動根據密度發現簇。

</details>

### 73. "序列模式挖掘 (Sequential Pattern Mining)" 與普通關聯規則的區別是？
A. 考慮項目出現的時間順序  
B. 不考慮順序  
C. 只用於文本  
D. 沒有區別

<details>
<summary>顯示答案</summary>

**正確答案：A**  
**解析：** 挖掘 "買了 A 之後接著買 B" 的模式。

</details>

### 74. 隨機森林中的 "OOB (Out-of-Bag) Error" 可用於？
A. 訓練模型  
B. 無需單獨驗證集即可評估模型性能  
C. 預測未來  
D. 增加樹的數量

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 利用未被抽樣到的數據進行自我驗證。

</details>

### 75. 梯度消失問題 (Vanishing Gradient) 常發生在？
A. 淺層網絡  
B. 使用 Sigmoid/Tanh 的深層神經網路  
C. 使用 ReLU 的網絡  
D. 決策樹

<details>
<summary>顯示答案</summary>

**正確答案：B**  
**解析：** 激活函數導數小於1，連乘後梯度趨近於0，導致前層參數無法更新。

</details>

---

## 答案總覽

| 題號 | 答案 | 題號 | 答案 | 題號 | 答案 | 題號 | 答案 |
|------|------|------|------|------|------|------|------|
| 1 | B | 21 | B | 41 | C | 61 | B |
| 2 | B | 22 | B | 42 | C | 62 | B |
| 3 | C | 23 | B | 43 | B | 63 | B |
| 4 | C | 24 | B | 44 | B | 64 | B |
| 5 | B | 25 | B | 45 | C | 65 | D |
| 6 | B | 26 | A | 46 | D | 66 | C |
| 7 | B | 27 | B | 47 | B | 67 | B |
| 8 | B | 28 | B | 48 | B | 68 | B |
| 9 | B | 29 | A | 49 | B | 69 | B |
| 10 | B | 30 | B | 50 | A | 70 | B |
| 11 | B | 31 | B | 51 | B | 71 | D |
| 12 | B | 32 | B | 52 | C | 72 | B |
| 13 | A | 33 | B | 53 | B | 73 | A |
| 14 | A | 34 | B | 54 | B | 74 | B |
| 15 | B | 35 | B | 55 | B | 75 | B |
| 16 | C | 36 | B | 56 | B | 70 | B |
| 17 | B | 37 | B | 57 | B | 71 | D |
| 18 | C | 38 | B | 58 | B | 72 | B |
| 19 | B | 39 | B | 59 | A | 73 | A |
| 20 | A | 40 | B | 60 | B | 74 | B |
| | | | | | | 75 | B |
