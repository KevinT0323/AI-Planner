# 大數據與機器學習 - 80題單選題題庫

基於 L22401_大數據與機器學習.md

考試日期：2026/05/23

---

## 第一部分：數據驅動的機器學習 (20題)

### 1. 機器學習界常說的 "Garbage In, Garbage Out" (GIGO) 意思是？
A. 數據越多越好  
B. 如果輸入數據品質低劣，無論模型多強大，輸出結果也會很差  
C. 模型可以自動修復錯誤數據  
D. 垃圾分類很重要

**正確答案：B**  
**解析：** 數據品質是機器學習的上限，模型只是逼近這個上限。

### 2. 一般而言，隨著數據量的增加，模型的性能通常會？
A. 下降，因為噪聲太多  
B. 保持不變  
C. 提升，尤其是對於高容量 (High Capacity) 的複雜模型 (如深度學習)  
D. 變得不穩定

**正確答案：C**  
**解析：** 大數據能提供更全面的樣本分佈，減少估計變異，特別是對深度學習這種「數據飢渴」的模型。

### 3. 當數據量極大時，機器學習模型傾向於發生什麼變化？
A. 更容易過擬合 (Overfitting)  
B. 更不容易過擬合，因為數據多樣性增加，模型需學習更通用的規律  
C. 訓練時間變短  
D. 模型變小

**正確答案：B**  
**解析：** 大數據相當於一種強力的正則化手段，強迫模型學習真正的模式而非噪聲。

### 4. 吳恩達 (Andrew Ng) 曾提出，數據與特徵工程在機器學習競賽中的地位是？
A. 沒什麼用  
B. 數據和特徵決定了性能的上限，模型和算法只是去逼近這個上限  
C. 算法最重要，數據無所謂  
D. 硬體最重要

**正確答案：B**  
**解析：** 強調了 Data-Centric AI 的重要性。

### 5. 對於傳統機器學習算法 (如 SVM, 邏輯回歸)，當數據量增加到一定程度後，性能通常會？
A. 持續線性增長  
B. 趨於飽和 (Plateau)，不再顯著提升  
C. 急劇下降  
D. 變成無限大

**正確答案：B**  
**解析：** 傳統淺層模型的容量有限，難以消化海量數據帶來的信息。

### 6. 深度學習 (Deep Learning) 為什麼需要大數據？
A. 因為它參數極多，需要大量數據來訓練以避免過擬合並學習特徵  
B. 為了展示硬體實力  
C. 因為它算得慢  
D. 為了以量取勝

**正確答案：A**  
**解析：** 深度神經網絡的高自由度（參數多）需要大量樣本約束。

### 7. 大數據幫助機器學習發現「複雜模式」是指？
A. 找出簡單的線性關係  
B. 挖掘出人類難以直觀察覺的非線性、高維度、細微的關聯  
C. 增加計算錯誤  
D. 發現數據的錯誤

**正確答案：B**  
**解析：** 例如在基因數據中發現致病模式，或在用戶行為中發現隱性興趣。

### 8. 「維度災難」在數據量不足時很嚴重，但在大數據場景下？
A. 依然是個問題，但可以通過海量數據填充空間來部分緩解，同時也帶來計算挑戰  
B. 完全消失  
C. 變得更糟  
D. 變成優勢

**正確答案：A**  
**解析：** 數據密度增加有助於緩解稀疏性，但高維度處理的運算壓力依然存在。

### 9. 什麼是特徵工程 (Feature Engineering) 的核心目標？
A. 增加數據量  
B. 將原始數據轉換為更能代表潛在問題的特徵，使模型更容易學習  
C. 刪除所有特徵  
D. 加密數據

**正確答案：B**  
**解析：** 好的特徵能讓簡單模型表現得像複雜模型一樣好。

### 10. 領域知識 (Domain Knowledge) 在特徵工程中的作用？
A. 不重要，AI 會自己學  
B. 非常重要，能指導我們構建有意義的特徵 (如 金融中的負債比率)  
C. 有害的  
D. 僅用於可視化

**正確答案：B**  
**解析：** 專家知識往往能轉化為強力的特徵，減少模型探索成本。

### 11. 數據增強 (Data Augmentation) 是指？
A. 購買更多數據  
B. 通過對現有數據進行變換 (如 旋轉圖片、增加噪聲) 來人工生成新數據  
C. 修復硬碟  
D. 壓縮數據

**正確答案：B**  
**解析：** 常用於圖像和語音領域，以低成本擴充數據集並提高模型魯棒性。

### 12. 非結構化數據 (如文本) 通常需要經過什麼處理才能用於機器學習？
A. 直接輸入  
B. 特徵提取與向量化 (如 Word2Vec, TF-IDF)  
C. 打印出來  
D. 刪除

**正確答案：B**  
**解析：** 機器學習模型本質上是數學運算，只能處理數值向量。

### 13. 大數據場景下，數據標註 (Labeling) 的瓶頸通常通過什麼解決？
A. 這不是問題  
B. 眾包 (Crowdsourcing)、半監督學習、主動學習 (Active Learning)  
C. 放棄標註  
D. 隨機標註

**正確答案：B**  
**解析：** 標註成本高昂，需要更聰明的策略來獲取標籤。

### 14. 數據分佈發生變化 (Data Drift) 會導致？
A. 模型性能隨時間衰退  
B. 模型性能提升  
C. 數據變多  
D. 沒有影響

**正確答案：A**  
**解析：** 訓練數據與線上數據分佈不一致，是模型部署後的主要風險。

### 15. 「長尾數據 (Long-tail Data)」對模型訓練的挑戰是？
A. 尾部類別樣本太少，模型難以學習  
B. 頭部數據太少  
C. 數據太長  
D. 計算太快

**正確答案：A**  
**解析：** 大部分樣本聚集在少數熱門類別，導致模型對冷門類別預測準確度低。

### 16. 高品質數據的一個特徵是？
A. 體積大  
B. 準確性、完整性、一致性、時效性  
C. 存儲在雲端  
D. 加密過

**正確答案：B**  
**解析：** 這是數據治理的標準維度。

### 17. 為什麼說特徵決定了模型的上限？
A. 為了嚇唬人  
B. 如果信息不在特徵中 (信息丟失)，在強的模型也猜不出來  
C. 特徵就是模型  
D. 特徵越多越好

**正確答案：B**  
**解析：** 巧婦難為無米之炊，信息含量是預測的基礎。

### 18. 在海量數據下，簡單模型 (如 線性回歸) 的優勢是？
A. 準確率最高  
B. 計算效率高，可解釋性強，且不易過擬合  
C. 不需要特徵工程  
D. 可以處理圖像

**正確答案：B**  
**解析：** 在廣告點擊率預測等超大規模場景，簡單模型（如 LR）依然是基石。

### 19. 什麼是「數據洩漏 (Data Leakage)」？
A. 數據庫被黑客入侵  
B. 訓練數據中包含了預測目標的信息 (如使用了未來特徵)，導致測試結果虛高  
C. 數據丟失  
D. 內存洩漏

**正確答案：B**  
**解析：** 這是機器學習中最常見也最致命的錯誤之一。

### 20. 大數據使得「端到端學習 (End-to-End Learning)」成為可能，這意味著？
A. 不需要人工特徵工程，直接從原始數據映射到輸出  
B. 需要更多特徵工程  
C. 模型首尾相連  
D. 只能在終端運行

**正確答案：A**  
**解析：** 如深度學習直接從像素識別物體，跳過了手工設計邊緣檢測等環節。

---

## 第二部分：分散式機器學習 (25題)

### 21. 「數據並行 (Data Parallelism)」是指？
A. 將數據複製到所有節點  
B. 將訓練數據切分到不同節點，每個節點持有一個完整的模型副本進行訓練  
C. 將模型切分  
D. 數據備份

**正確答案：B**  
**解析：** 這是最常用的並行策略，每個節點算一部分數據的梯度，然後匯總。

### 22. 「模型並行 (Model Parallelism)」適用於？
A. 模型參數極大，單卡顯存/內存放不下 (如 GPT-3)  
B. 數據量極大  
C. 網絡很快  
D. 模型很小

**正確答案：A**  
**解析：** 將神經網絡的不同層或不同部分分配到不同設備上計算。

### 23. 參數伺服器 (Parameter Server) 架構的主要角色是？
A. Worker (計算梯度) 和 Server (存儲與更新參數)  
B. Client 和 Server  
C. Master 和 Slave  
D. Map 和 Reduce

**正確答案：A**  
**解析：** Worker 負責算，Server 負責記，是分散式 ML 的經典架構。

### 24. 在分散式訓練中，梯度的「同步更新 (Synchronous Update)」是指？
A. 沒人等待  
B. 所有 Worker 都算完這輪梯度後，才匯總更新參數，再進行下一輪  
C. 隨到隨更  
D. 不更新

**正確答案：B**  
**解析：** 優點是收斂穩定（等同於大 Batch），缺點是受限於最慢的節點 (木桶效應)。

### 25. 「非同步更新 (Asynchronous Update)」的主要優點是？
A. 精度最高  
B. 速度快，無等待開銷 (Worker 算完就提交，不需要等別人)  
C. 容易實現  
D. 節省帶寬

**正確答案：B**  
**解析：** 缺點是會引入「梯度陳舊 (Stale Gradients)」問題，影響收斂。

### 26. Ring All-Reduce 算法旨在解決什麼問題？
A. 參數伺服器的帶寬瓶頸  
B. 內存不足  
C. 數據不平衡  
D. 計算太慢

**正確答案：A**  
**解析：** 通過環狀傳遞，讓帶寬利用率最優化，無需中心化的瓶頸節點。

### 27. 「梯度累積 (Gradient Accumulation)」技術允許我們？
A. 在顯存有限的情況下，通過多次前向/反向傳播累積梯度，模擬大 Batch Size 訓練  
B. 加快計算速度  
C. 減少數據量  
D. 自動調參

**正確答案：A**  
**解析：** 用時間換空間，解決顯存不足無法開大 Batch 的問題。

### 28. 混合精度訓練 (Mixed Precision Training) 通常使用哪兩種格式？
A. FP32 和 FP64  
B. FP16 (半精度) 和 FP32 (單精度)  
C. INT8 和 INT16  
D. 字符串

**正確答案：B**  
**解析：** 計算和存儲用 FP16 加速且省顯存，關鍵參數更新用 FP32 保持精度。

### 29. 混合精度訓練中「Loss Scaling」的作用是？
A. 放大 Loss，防止 FP16 下梯度過小導致數值下溢 (Underflow)  
B. 縮小 Loss  
C. 增加權重  
D. 刪除梯度

**正確答案：A**  
**解析：** FP16 的表示範圍小，微小的梯度可能會變成 0，放大後可保留精度。

### 30. 在分散式環境下，通信開銷 (Communication Overhead) 通常發生在？
A. 讀取數據時  
B. 梯度匯總與參數同步時  
C. 打印日誌時  
D. 啟動時

**正確答案：B**  
**解析：** 這是限制分散式擴展效率的主要瓶頸。

### 31. Straggler (落後者/慢節點) 問題對哪種更新策略影響最大？
A. 非同步更新  
B. 同步更新 (BSP)  
C. 本地更新  
D. 無影響

**正確答案：B**  
**解析：** 所有人都得等最慢的那個，嚴重拖累整體速度。

### 32. Horovod 是一個流行的分散式訓練框架，它主要基於什麼算法？
A. Parameter Server  
B. Ring All-Reduce  
C. MapReduce  
D. Goku

**正確答案：B**  
**解析：** 由 Uber 開源，基於 MPI 和 NCCL，易於與 TF/PyTorch 集成。

### 33. 對於超大規模稀疏模型 (如 推薦系統 Embedding)，通常採用？
A. 僅數據並行  
B. 參數伺服器 (PS) 架構，因為 Embedding 表太大無法放入單卡  
C. 單機訓練  
D. All-Reduce

**正確答案：B**  
**解析：** PS 架構特別適合存儲並查詢巨大的稀疏參數矩陣。

### 34. 聯邦學習 (Federated Learning) 是一種特殊的分散式學習，它的特點是？
A. 數據匯總到中心  
B. 數據不出本地 (如 用戶手機)，只上傳加密梯度/模型更新  
C. 公開所有數據  
D. 不訓練模型

**正確答案：B**  
**解析：** 隱私保護下的分散式訓練。

### 35. 數據並行中，如果 Batch Size 變得非常大，可能需要調整？
A. 學習率 (Learning Rate Scaling)  
B. 網絡帶寬  
C. 硬碟大小  
D. 顯示器

**正確答案：A**  
**解析：** 根據 Linear Scaling Rule，Batch 變大，LR 也應成比例變大。

### 36. 學習率預熱 (Warmup) 是指？
A. 訓練開始前先讓 GPU 熱機  
B. 訓練初期使用較小的學習率，逐漸增加到設定值，以穩定訓練  
C. 增加溫度  
D. 減少數據

**正確答案：B**  
**解析：** 防止大 LR 在訓練初期導致模型震盪或發散。

### 37. 在大規模集群中，「容錯 (Fault Tolerance)」意味著？
A. 不能出錯  
B. 當某個節點掛掉時，系統能自動恢復 (如從 Checkpoint 恢復) 而不需重頭開始  
C. 允許計算錯誤  
D. 自動關機

**正確答案：B**  
**解析：** 節點越多，故障機率越高，Checkpoints 至關重要。

### 38. GPipe 是一種什麼技術？
A. 數據管道  
B. 流水線並行 (Pipeline Parallelism)，用於訓練超大模型  
C. 圖像處理  
D. 壓縮算法

**正確答案：B**  
**解析：** 將模型切分後，利用流水線技術減少設備空閒時間 (Bubble)。

### 39. 梯度檢查點 (Gradient Checkpointing) 用於？
A. 檢查梯度是否正確  
B. 以計算換顯存 (重算部分前向傳播)，從而訓練更大的模型  
C. 保存模型  
D. 停止訓練

**正確答案：B**  
**解析：** 不保存所有中間激活值，反向傳播時臨時重算，節省顯存。

### 40. ZeRO (Zero Redundancy Optimizer) 技術的核心是？
A. 數據歸零  
B. 消除數據並行中的內存冗餘 (分片 Optimizer States, Gradients, Parameters)  
C. 零樣本學習  
D. 零延遲

**正確答案：B**  
**解析：** DeepSpeed 的核心技術，讓普通 GPU 集群也能訓練大模型。

### 41. 同步 SGD 的一個變體 Soft-Sync 是為了？
A. 讓同步更柔和  
B. 允許忽略極少數極慢的 Stragglers，達到一定比例 (如 95%) 即更新  
C. 完全不同步  
D. 軟體同步

**正確答案：B**  
**解析：** 在同步與非同步之間的折衷。

### 42. 分散式訓練中的「全局 Batch Size」等於？
A. 單卡 Batch Size  
B. 單卡 Batch Size × 節點數 × 每節點卡數  
C. 樣本總數  
D. 0

**正確答案：B**  
**解析：** 理解這一點對調整學習率至關重要。

### 43. NVLink 技術有助於解決什麼瓶頸？
A. CPU 計算  
B. 使用 PCIe 導致的 GPU 間通信帶寬瓶頸  
C. 硬碟 I/O  
D. 網速

**正確答案：B**  
**解析：** NVIDIA 的高速互連技術，大幅提升多卡並行效率。

### 44. TFRecord 格式在 TensorFlow 中常用是因為？
A. 它是二進制格式，讀取效率高，適合大數據流式讀取  
B. 它是文本格式，易讀  
C. 它支持加密  
D. 它是圖片格式

**正確答案：A**  
**解析：** 減少 I/O 開銷，避免小文件問題。

### 45. 分散式推理 (Distributed Inference) 通常關注？
A. 梯度更新  
B. 延遲 (Latency) 和 吞吐量 (Throughput)  
C. 訓練損失  
D. 反向傳播

**正確答案：B**  
**解析：** 線上服務更看重響應速度。

---

## 第三部分：特徵工程與降維 (20題)

### 46. PCA (主成分分析) 是一種？
A. 線性降維技術，最大化數據變異  
B. 非線性降維  
C. 聚類算法  
D. 分類算法

**正確答案：A**  
**解析：** 將數據投影到變異數最大的正交方向上。

### 47. t-SNE 相比 PCA，更適合？
A. 數據壓縮  
B. 高維數據的可視化，保留局部結構 (流形學習)  
C. 預處理  
D. 特徵選擇

**正確答案：B**  
**解析：** t-SNE 能很好地展示簇的結構，但計算複雜度高，不適合轉換新數據。

### 48. 特徵選擇中的「過濾法 (Filter Method)」是？
A. 使用模型訓練來選擇  
B. 根據統計指標 (如 相關係數、卡方檢驗) 對特徵評分排序，獨立於模型  
C. 隨機選擇  
D. 包裝法

**正確答案：B**  
**解析：** 速度快，作為預處理步驟。

### 49. 「包裝法 (Wrapper Method)」(如 遞歸特徵消除 RFE) 的特點是？
A. 速度最快  
B. 將特徵選擇看作搜索問題，使用模型性能作為評估標準，效果好但計算貴  
C. 不使用模型  
D. 隨機猜測

**正確答案：B**  
**解析：** 針對特定模型優化特徵子集，容易過擬合且慢。

### 50. 「嵌入法 (Embedded Method)」(如 Lasso 回歸, 決策樹) 是？
A. 在模型訓練過程中自動進行特徵選擇 (如 L1 正則化使係數歸零)  
B. 嵌入圖片  
C. 手動選擇  
D. 過濾法

**正確答案：A**  
**解析：** 結合了 Filter 和 Wrapper 的優點。

### 51. One-Hot Encoding (獨熱編碼) 處理高基數 (High Cardinality) 類別特徵時的問題是？
A. 計算快  
B. 導致特徵空間極度稀疏且維度爆炸  
C. 準確度高  
D. 沒有問題

**正確答案：B**  
**解析：** 類別太多會產生幾萬個 0 和一個 1，浪費空間且難學。

### 52. 針對高基數類別特徵，更好的處理方法是？
A. 刪除  
B. Embedding (嵌入) 或 Target Encoding (目標編碼)  
C. 獨熱編碼  
D. 賦予隨機數

**正確答案：B**  
**解析：** Embedding 將其映射為低維稠密向量。

### 53. 特徵交叉 (Feature Crossing) 的目的是？
A. 減少特徵  
B. 捕捉特徵之間的交互作用 (非線性關係)  
C. 增加噪聲  
D. 加密

**正確答案：B**  
**解析：** 如「女性」且「喜歡化妝品」，組合特徵往往比單獨特徵更有預測力。

### 54. 因子分解機 (FM, Factorization Machines) 解決了什麼問題？
A. 圖像分類  
B. 稀疏數據下的特徵組合問題 (通過向量內積模擬組合權重)  
C. 文本生成  
D. 排序

**正確答案：B**  
**解析：** 相比多項式回歸，FM 能在稀疏數據下泛化學習二階交互。

### 55. Wide & Deep 模型的 "Wide" 部分通常負責？
A. 記憶 (Memorization) 歷史出現過的共現特徵  
B. 泛化 (Generalization)  
C. 圖像處理  
D. 語音識別

**正確答案：A**  
**解析：** Wide 記住規則，Deep 負責泛化。

### 56. 歸一化 (Normalization) 和 標準化 (Standardization) 的主要目的是？
A. 讓所有特徵具有相同的尺度，加速梯度下降收斂  
B. 為了好看  
C. 增加計算量  
D. 改變數據分佈形狀

**正確答案：A**  
**解析：** 不同量綱的特徵會讓 Loss Surface 變成橢圓，導致優化困難。

### 57. 處理缺失值 (Missing Values) 的高級方法不包括？
A. 均值填補  
B. 模型預測填補 (如 KNN, Regression)  
C. 直接丟棄 (如果缺失很多且無規律)  
D. 將缺失作為一種特徵 (如 新增 Is_Missing 列)

**正確答案：A**  
**解析：** 嚴格來說 A 是最基礎的方法，不算高級；D 常常很有效，因為缺失本身就是信息。

### 58. 什麼是「特徵雜湊 (Feature Hashing / Hashing Trick)」？
A. 加密  
B. 將高維特徵映射到固定維度的向量空間，解決維度不確定問題  
C. 排序  
D. 壓縮

**正確答案：B**  
**解析：** 犧牲少量衝突換取空間和效率，常用於在線學習。

### 59. 圖像數據的特徵提取通常依賴？
A. 手工設計 (如 SIFT, HOG)  
B. 卷積神經網絡 (CNN) 自動提取  
C. 像素點相加  
D. 隨機生成

**正確答案：B**  
**解析：** 深度學習時代，CNN 已經取代了大部分手工特徵。

### 60. 文本數據的 TF-IDF 特徵表示？
A. 詞頻  
B. 詞頻 x 逆文檔頻率 (衡量詞對文檔的獨特重要性)  
C. 詞向量  
D. 詞性

**正確答案：B**  
**解析：** 降低常見詞 (如 "的", "是") 的權重。

### 61. Word2Vec 屬於哪種特徵工程技術？
A. 文本向量化 (Embedding)  
B. 圖像處理  
C. 降維  
D. 聚類

**正確答案：A**  
**解析：** 將詞映射為語義空間的稠密向量。

### 62. 在時間序列中，窗口統計特徵 (Rolling Window Statistics) 指的是？
A. 未來數據  
B. 滑動窗口內的均值、最大值、變異數等  
C. 隨機數  
D. 靜態特徵

**正確答案：B**  
**解析：** 捕捉時間序列的局部趨勢和波動。

### 63. 信息增益 (Information Gain) 常用於？
A. 回歸分析  
B. 決策樹節點分裂時的特徵選擇標準  
C. 聚類  
D. 降維

**正確答案：B**  
**解析：** 衡量特徵對降低熵 (不確定性) 的貢獻。

### 64. 為什麼說決策樹不需要歸一化？
A. 因為它基於閾值切割空間，只看大小關係，不看數值距離  
B. 它自動歸一化  
C. 它不計算梯度  
D. 它很聰明

**正確答案：A**  
**解析：** 樹模型對單調變換不敏感。

### 65. 什麼是相關性分析中的「多重共線性 (Multicollinearity)」？
A. 特徵太多  
B. 特徵之間高度相關，導致模型參數估計不穩定  
C. 特徵與目標無關  
D. 數據重複

**正確答案：B**  
**解析：** 需要去除相關性過高的冗餘特徵。

---

## 第四部分：進階概念與應用 (15題)

### 66. 自動機器學習 (AutoML) 的目標是？
A. 取代所有數據科學家  
B. 自動化特徵選擇、模型選擇和與超參數調優的過程  
C. 自動寫代碼  
D. 自動收集數據

**正確答案：B**  
**解析：** 降低機器學習門檻，提升效率。

### 67. 在推薦系統中，冷啟動問題通常不通過哪種方式解決？
A. 利用用戶註冊信息  
B. 利用熱門榜單  
C. 直接使用協同過濾 (因為沒有歷史行為)  
D. 這是一個陷阱題

**正確答案：C**  
**解析：** 協同過濾依賴歷史行為，冷啟動正是因為沒有行為，所以失效。

### 68. 知識蒸餾 (Knowledge Distillation) 是指？
A. 提煉數據  
B. 用一個大模型 (Teacher) 教導一個小模型 (Student)，讓小模型達到接近大模型的性能  
C. 壓縮圖片  
D. 刪除模型

**正確答案：B**  
**解析：** 模型壓縮與加速的重要技術。

### 69. 什麼是「特徵重要性 (Feature Importance)」？
A. 特徵的數量  
B. 模型訓練後給出的指標，衡量特徵對預測結果的貢獻程度  
C. 特徵的長度  
D. 特徵的類型

**正確答案：B**  
**解析：** 如 XGBoost 的 `gain` 或 Shuffle Importance，用於可解釋性。

### 70. 數據漂移中的 "Concept Drift" 是指？
A. 輸入數據分佈變了 P(X)  
B. 輸入與輸出的映射關係變了 P(Y|X) (如 用戶興趣變了)  
C. 數據格式變了  
D. 代碼變了

**正確答案：B**  
**解析：** 最難檢測的漂移類型。

### 71. 在大規模廣告點擊預測 (CTR) 中，最核心的技術挑戰是？
A. 圖像識別  
B. 處理極高維稀疏特徵 (High-dimensional Sparse Features)  
C. 語音生成  
D. 文本翻譯

**正確答案：B**  
**解析：** 數十億的 Feature ID。

### 72. 什麼是「在線學習 (Online Learning)」？
A. 在網上學習  
B. 模型隨數據流不斷增量更新，適應最新趨勢  
C. 離線訓練  
D. 看視頻學習

**正確答案：B**  
**解析：** 即時性最高的學習方式，如 FTRL 算法。

### 73. A/B 測試在機器學習上線流程中的作用？
A. 測試代碼 bug  
B. 在真實流量中對比新舊模型的業務效果 (如 點擊率、轉化率)  
C. 壓力測試  
D. 單元測試

**正確答案：B**  
**解析：** 離線指標好不代表線上效果好，必須做 A/B Test。

### 74. 模型量化 (Quantization) 將 FP32 轉為 INT8 的好處是？
A. 模型體積變小，推理速度變快，更適合移動端部署  
B. 精度變高  
C. 訓練變快  
D. 沒好處

**正確答案：A**  
**解析：** 邊緣計算的關鍵技術。

### 75. 圖神經網絡 (GNN) 擅長處理？
A. 圖片  
B. 非歐幾里得空間數據 (如 社交網絡、化學分子結構)  
C. 文本  
D. 聲音

**正確答案：B**  
**解析：** 處理節點與邊的關係。

### 76. 什麼是 SHAP 值？
A. 一種模型  
B. 一種基於博弈論的特徵歸因方法，用於解釋模型預測  
C. 數據格式  
D. 優化器

**正確答案：B**  
**解析：** 目前最流行的模型可解釋性 (XAI) 工具之一。

### 77. 在大數據特徵工程中，"Binning" (分箱) 對線性模型的作用？
A. 沒用  
B. 引入非線性能力，並對異常值魯棒  
C. 降低維度  
D. 增加計算

**正確答案：B**  
**解析：** 將連續變數離散化，讓線性模型能擬合分段非線性關係。

### 78. 什麼是 "Data Parallelism" 中的 "Gradient Noise"？
A. 噪音  
B. 由於每個節點只看一部分數據，其計算的梯度是對真實梯度的有噪估計  
C. 喇叭聲音  
D. 錯誤數據

**正確答案：B**  
**解析：** Batch Size 越大，噪音越小，但泛化能力可能變差。

### 79. MapReduce 適合迭代式的機器學習訓練嗎？
A. 非常適合  
B. 不適合，因為每輪迭代都要頻繁讀寫磁碟 (I/O 開銷大)  
C. 沒區別  
D. 是唯一的選擇

**正確答案：B**  
**解析：** 這就是為什麼 Spark (基於內存) 取代了 Hadoop MapReduce 用於 ML。

### 80. 大數據機器學習的未來趨勢是？
A. 模型越來越小  
B. 大模型 (Foundation Models)、多模態、自監督學習  
C. 放棄數據  
D. 手工規則

**正確答案：B**  
**解析：** 邁向通用人工智能 (AGI) 的方向。

---

## 答案總覽

| 題號 | 答案 | 題號 | 答案 | 題號 | 答案 | 題號 | 答案 |
|------|------|------|------|------|------|------|------|
| 1 | B | 21 | B | 41 | B | 61 | A |
| 2 | C | 22 | A | 42 | B | 62 | B |
| 3 | B | 23 | A | 43 | B | 63 | B |
| 4 | B | 24 | B | 44 | A | 64 | A |
| 5 | B | 25 | B | 45 | B | 65 | B |
| 6 | A | 26 | A | 46 | A | 66 | B |
| 7 | B | 27 | A | 47 | B | 67 | C |
| 8 | A | 28 | B | 48 | B | 68 | B |
| 9 | B | 29 | A | 49 | B | 69 | B |
| 10 | B | 30 | B | 50 | A | 70 | B |
| 11 | B | 31 | B | 51 | B | 71 | B |
| 12 | B | 32 | B | 52 | B | 72 | B |
| 13 | B | 33 | B | 53 | B | 73 | B |
| 14 | A | 34 | B | 54 | B | 74 | A |
| 15 | A | 35 | A | 55 | A | 75 | B |
| 16 | B | 36 | B | 56 | A | 76 | B |
| 17 | B | 37 | B | 57 | A | 77 | B |
| 18 | B | 38 | B | 58 | B | 78 | B |
| 19 | B | 39 | B | 59 | B | 79 | B |
| 20 | A | 40 | B | 60 | B | 80 | B |
