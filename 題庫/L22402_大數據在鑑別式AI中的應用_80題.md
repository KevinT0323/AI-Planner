# 大數據在鑑別式AI中的應用 - 80題單選題題庫

基於 L22402_大數據在鑑別式AI中的應用.md

考試日期：2026/05/23

---

## 第一部分：圖像分類與計算機視覺 (20題)

### 1. 鑑別式 AI (Discriminative AI) 的主要目標是？
A. 生成新的數據  
B. 在不同類別的數據之間劃分邊界 (分類) 或預測數值  
C. 壓縮數據  
D. 存儲數據

**正確答案：B**  
**解析：** 鑑別模型學習的是 P(Y|X)，即給定輸入 X，判斷它是類別 Y 的機率，如貓或狗。

### 2. ImageNet 是一個著名的？
A. 圖像生成工具  
B. 包含超過 1400 萬張手動標註圖片的大規模圖像數據集  
C. 圖像壓縮格式  
D. 顯卡型號

**正確答案：B**  
**解析：** ImageNet 是推動深度學習革命的基石數據集。

### 3. 在大數據圖像分類中，數據增強 (Data Augmentation) 的作用是？
A. 減少圖片大小  
B. 通過旋轉、裁剪、翻轉等操作增加數據多樣性，防止過擬合  
C. 增加圖片解析度  
D. 刪除模糊圖片

**正確答案：B**  
**解析：** 讓模型學習到物體在不同視角和環境下的不變性。

### 4. 遷移學習 (Transfer Learning) 在計算機視覺中的典型應用是？
A. 從零開始訓練  
B. 使用在 ImageNet 上預訓練的模型 (如 ResNet) 作為特徵提取器，在特定任務上微調  
C. 合併兩張圖片  
D. 無監督學習

**正確答案：B**  
**解析：** 利用預訓練模型學到的通用特徵 (邊緣、紋理)，解決小樣本問題。

### 5. ResNet (殘差網絡) 的主要創新解決了什麼問題？
A. 計算慢  
B. 深度神經網絡中的梯度消失問題，使得訓練超深網絡 (如 100層+) 成為可能  
C. 圖像模糊  
D. 內存不足

**正確答案：B**  
**解析：** 通過 Skip Connection 讓梯度能直接流向底層。

### 6. COCO 數據集常用於？
A. 只有分類  
B. 物件檢測 (Object Detection)、分割 (Segmentation) 和 字幕生成 (Captioning)  
C. 語音識別  
D. 文本翻譯

**正確答案：B**  
**解析：** 微軟的 COCO 數據集標註更精細，包含物體位置和輪廓。

### 7. VGG 網絡的特點是？
A. 結構非常複雜  
B. 使用小卷積核 (3x3) 和深層結構，結構簡潔統一  
C. 參數量極少  
D. 沒有全連接層

**正確答案：B**  
**解析：** VGG 證明了網絡深度的重要性。

### 8. EfficientNet 的設計理念是？
A. 只增加深度  
B. 同時對網絡的深度、寬度和解析度進行複合縮放 (Compound Scaling)，以達到效率和精度的平衡  
C. 隨機設計  
D. 只增加寬度

**正確答案：B**  
**解析：** 用更少的參數和計算量達到更高的準確率。

### 9. 卷積神經網絡 (CNN) 中的「池化層 (Pooling)」作用是？
A. 增加參數  
B. 降低特徵圖的維度 (下採樣)，減少計算量，並保持平移不變性  
C. 增加圖像亮度  
D. 分類

**正確答案：B**  
**解析：** 常見的有最大池化 (Max Pooling)。

### 10. 在物件檢測中，「IoU (Intersection over Union)」用於？
A. 計算速度  
B. 評估預測框與真實框的重疊程度 (交集/聯集)  
C. 裁剪圖片  
D. 調整顏色

**正確答案：B**  
**解析：** 衡量檢測準確度的關鍵指標。

### 11. YOLO (You Only Look Once) 是一種？
A. 圖像分類算法  
B. 實時物件檢測算法，將檢測問題轉化為回歸問題  
C. 生成模型  
D. 聊天機器人

**正確答案：B**  
**解析：** 速度極快，適合視頻流檢測。

### 12. 語義分割 (Semantic Segmentation) 的目標是？
A. 畫框框  
B. 對圖像中的每一個像素進行分類 (像素級分類)  
C. 識別文字  
D. 生成圖像

**正確答案：B**  
**解析：** 區分出圖像中每個區域屬於什麼類別 (如 天空、道路)。

### 13. 與鑑別式 AI 相對的是？
A. 強化學習  
B. 生成式 AI (Generative AI)  
C. 線性回歸  
D. 決策樹

**正確答案：B**  
**解析：** 生成式 AI 學習聯合機率分佈 P(X, Y) 以創造新樣本。

### 14. 圖像分類中，Top-5 錯誤率是指？
A. 最大的 5 個錯誤  
B. 正確標籤不在模型預測機率最高的前 5 個類別中的比例  
C. 前 5 張圖片都錯了  
D. 準確率為 5%

**正確答案：B**  
**解析：** 常用於類別非常多 (如 1000 類) 的數據集評估。

### 15. 在大量圖片中檢索相似圖片，通常使用？
A. 像素對比  
B. 圖像特徵向量的相似度計算 (如 餘弦相似度)  
C. 文件名搜索  
D. 人工查看

**正確答案：B**  
**解析：** 將圖片 Embedding 成向量進行搜索。

### 16. 弱監督學習 (Weakly Supervised Learning) 在圖像領域的優勢？
A. 準確度最高  
B. 可以利用大量標籤不精確 (如只有圖像級標籤，沒有邊界框) 的數據進行訓練  
C. 不需要任何標籤  
D. 只能處理小圖

**正確答案：B**  
**解析：** 降低了昂貴的細粒度標註成本。

### 17. 圖像預處理中的「歸一化」通常是將像素值？
A. 放大到 0-255  
B. 縮放到 0-1 或 -1 到 1 之間  
C. 隨機化  
D. 轉為黑白

**正確答案：B**  
**解析：** 加速模型收斂。

### 18. 主動學習 (Active Learning) 在圖像標註中的策略是？
A. 標註所有圖片  
B. 模型主動挑選它「最不確定」的樣本交給人工標註  
C. 隨機標註  
D. 不標註

**正確答案：B**  
**解析：** 以最小的標註成本獲得最大的性能提升。

### 19. 卷積核 (Filter/Kernel) 在 CNN 中學習的是？
A. 固定的圖案  
B. 特徵檢測器 (如 邊緣、角點、紋理)  
C. 圖片的顏色  
D. 圖片的標籤

**正確答案：B**  
**解析：** 低層學邊緣，高層學語義部件。

### 20. 自監督學習 (Self-Supervised Learning) 如 SimCLR, MAE 在計算機視覺中的崛起是因為？
A. 標註數據太貴，它能從海量無標註圖像中學習通用特徵表示  
B. 它比監督學習更簡單  
C. 它可以生成圖像  
D. Google 在推

**正確答案：A**  
**解析：** 預訓練範式逐漸從有監督轉向自監督。

---

## 第二部分：文本分類與自然語言處理 (20題)

### 21. Word2Vec 的核心思想是？
A. 詞袋模型  
B. 分布式假設 (Distributed Hypothesis)：上下文相似的詞，其語義也相似  
C. 語法規則  
D. 字典查找

**正確答案：B**  
**解析：** 用向量距離表示語義距離。

### 22. BERT 模型中的 "Bidirectional" (雙向) 意味著？
A. 只能從左到右讀  
B. 同時關注一個詞左右兩側的上下文信息  
C. 兩種語言  
D. 輸入和輸出

**正確答案：B**  
**解析：** 真正理解語境，而非單向預測。

### 23. 大規模文本分類任務 (如 新聞分類) 通常的第一步是？
A. 文本清洗與分詞 (Tokenization)  
B. 訓練模型  
C. 部署  
D. 預測

**正確答案：A**  
**解析：** 將非結構化文本轉化為結構化單元。

### 24. TF-IDF 在文本分類中常用於？
A. 生成文本  
B. 提取關鍵詞特徵，降低常見停用詞的權重  
C. 翻譯  
D. 語音合成

**正確答案：B**  
**解析：** 傳統且有效的文本特徵表示。

### 25. Common Crawl 是一個？
A. 爬蟲軟體  
B. 開放的大規模網頁數據集，包含數十億網頁，常用於訓練大語言模型  
C. 搜索引擎  
D. 病毒

**正確答案：B**  
**解析：** LLM 的主要食物來源。

### 26. GloVe 與 Word2Vec 的區別在於？
A. GloVe 結合了全局矩陣分解與局部上下文窗口  
B. GloVe 是生成式模型  
C. GloVe 只能處理英文  
D. Word2Vec 更慢

**正確答案：A**  
**解析：** 兩者都是詞嵌入技術，但優化目標不同。

### 27. 情感分析 (Sentiment Analysis) 是文本分類的一種，它主要識別？
A. 文本的主題  
B. 文本的情緒傾向 (正向/負向/中性)  
C. 作者的性別  
D. 發布時間

**正確答案：B**  
**解析：** 廣泛應用於輿情監控和產品評論分析。

### 28. 所謂 "Stop Words" (停用詞) 是指？
A. 停止標誌  
B. 在文本中頻繁出現但對語義貢獻很小的詞 (如 "的", "是", "the")  
C. 敏感詞  
D. 關鍵詞

**正確答案：B**  
**解析：** 通常在預處理階段被過濾掉。

### 29. N-gram 模型中的 N=2 (Bigram) 表示？
A. 無限長的序列  
B. 相鄰的兩個詞組成的序列  
C. 兩個句子  
D. 兩個字母

**正確答案：B**  
**解析：** 捕捉局部詞序信息。

### 30. Transformer 架構中的 Self-Attention 機制解決了 RNN 的什麼短板？
A. 參數太少  
B. 無法並行計算且難以捕捉長距離依賴  
C. 只能處理短文本  
D. 內存佔用大

**正確答案：B**  
**解析：** 並行化能力和全局視野是 Transformer 成功的關鍵。

### 31. 垃圾郵件檢測通常屬於哪種 NLP 任務？
A. 機器翻譯  
B. 二元文本分類  
C. 文本摘要  
D. 問答系統

**正確答案：B**  
**解析：** 判斷郵件是或不是垃圾郵件。

### 32. 多語言模型 (如 mBERT) 的優勢是？
A. 只能說英語  
B. 用一種語言微調後，能直接遷移到其他語言 (Zero-shot Cross-lingual Transfer)  
C. 需要為每種語言單獨訓練  
D. 參數量小

**正確答案：B**  
**解析：** 共享的向量空間實現了跨語言能力。

### 33. 命名實體識別 (NER) 的目標是？
A. 翻譯句子  
B. 從文本中識別出特定類型的實體 (如 人名、地名、組織機構)  
C. 改寫句子  
D. 糾正錯別字

**正確答案：B**  
**解析：** 信息抽取的重要步驟。

### 34. 大語言模型 (LLM) 用於分類任務時，通常採用的新範式是？
A. 從頭訓練  
B. Prompt Engineering (提示工程) 或 Instruction Tuning (指令微調)  
C. 隨機猜測  
D. 只做生成

**正確答案：B**  
**解析：** 激發預訓練模型已有的知識來解決特定任務。

### 35. 詞嵌入 (Word Embedding) 將詞映射到？
A. 整數 ID  
B. 獨熱編碼  
C. 低維稠密實數向量空間  
D. 哈希值

**正確答案：C**  
**解析：** 捕捉詞與詞之間的語義關係。

### 36. 文本數據中的 "Token" 通常指？
A. 貨幣  
B. 文本處理的最小單位 (詞、字或子詞 Subword)  
C. 密碼  
D. 整句話

**正確答案：B**  
**解析：** Tokenization 是 NLP 的第一步。

### 37. 文本增強 (Text Augmentation) 的常見方法不包括？
A. 同義詞替換  
B. 回譯 (Back-translation)  
C. 隨機刪除  
D. 旋轉文本 (這是圖像的)

**正確答案：D**  
**解析：** 文本沒有旋轉的概念。

### 38. 在 NLP 中，"Corpus" 是指？
A. 屍體  
B. 語料庫，即大量的文本數據集合  
C. 代碼庫  
D. 字典

**正確答案：B**  
**解析：** 訓練模型的基礎素菜。

### 39. 為什麼 LSTM 適合處理文本？
A. 它是卷積網絡  
B. 它有記憶單元，能處理序列數據的時間依賴性  
C. 它不需要訓練  
D. 它只看當前詞

**正確答案：B**  
**解析：** 文本本質上是時間序列數據。

### 40. 上下文理解意味著模型能區分？
A. "蘋果" 是水果還是手機品牌 (多義詞消歧)  
B. 字體大小  
C. 顏色  
D. 語言種類

**正確答案：A**  
**解析：** 根據周圍的詞來確定當前詞的含義。

---

## 第三部分：語音識別與檢測 (20題)

### 41. 語音識別 (ASR) 的目標是？
A. 將語音轉化為文本 (Speech-to-Text)  
B. 將文本轉化為語音 (TTS)  
C. 識別說話人是誰  
D. 翻譯語言

**正確答案：A**  
**解析：** Automatic Speech Recognition.

### 42. LibriSpeech 是一個？
A. 電子書閱讀器  
B. 大規模開源英語語音數據集 (基於有聲書)  
C. 語音助手  
D. 音樂庫

**正確答案：B**  
**解析：** ASR 領域的標準基準數據集。

### 43. 傳統語音識別包含聲學模型 (Acoustic Model) 和語言模型 (Language Model)，其中語言模型負責？
A. 識別聲音特徵  
B. 預測詞序列的機率 (即這句話是否通順)  
C. 錄音  
D. 降噪

**正確答案：B**  
**解析：** 糾正聲學模型識別出的同音異義詞錯誤。

### 44. 端到端 (End-to-End) 語音識別模型的特點是？
A. 需要複雜的流水線 (對齊、音素建模等)  
B. 直接將語音波形或頻譜特徵映射到文本序列  
C. 準確率低  
D. 編碼困難

**正確答案：B**  
**解析：** 簡化了訓練流程，如 DeepSpeech, Whisper。

### 45. 梅爾頻率倒譜係數 (MFCC) 是？
A. 一種音頻格式  
B. 語音信號處理中常用的特徵提取方法 (模擬人耳聽覺)  
C. 加密算法  
D. 壓縮算法

**正確答案：B**  
**解析：** 傳統語音識別的核心特徵。

### 46. Common Voice 是由 Mozilla 推出的項目，其特點是？
A. 收費  
B. 公眾捐贈語音數據，推動開源和多語言語音技術  
C. 專注於音樂  
D. 僅限英語

**正確答案：B**  
**解析：** 強調多樣性和開放性。

### 47. 雞尾酒會問題 (Cocktail Party Problem) 指的是？
A. 調酒技術  
B. 盲源分離 (Blind Source Separation)，即從混合聲音中分離出特定說話人的聲音  
C. 語音合成  
D. 噪音消除

**正確答案：B**  
**解析：** 鑑別式 AI 在語音分離上的經典難題。

### 48. 聲紋識別 (Speaker Verification) 與 語音識別的區別？
A. 沒區別  
B. 聲紋識別是確認 "誰在說"，語音識別是確認 "說了什麼"  
C. 聲紋識別不需要訓練  
D. 語音識別更難

**正確答案：B**  
**解析：** 一個是生物特徵認證，一個是內容轉錄。

### 49. 連接時序分類 (CTC) 損失函數常用於？
A. 圖像分類  
B. 解決語音序列和文本序列長度不一致且未對齊的問題  
C. 文本生成  
D. 聚類

**正確答案：B**  
**解析：** 端到端語音識別的關鍵技術。

### 50. 語音活動檢測 (VAD) 的作用是？
A. 檢測語音的情緒  
B. 判斷音頻片段中是否存在人聲 (去除靜音段)  
C. 識別語言  
D. 增加音量

**正確答案：B**  
**解析：** 減少無效處理，提高效率。

### 51. OpenAI 的 Whisper 模型是一個？
A. 閉源模型  
B. 強大的多語言、多任務 (識別+翻譯) 語音識別模型  
C. 圖像生成模型  
D. 聊天機器人

**正確答案：B**  
**解析：** 展現了大規模弱監督訓練在語音領域的威力。

### 52. 語音數據增強的常見方法？
A. 旋轉頻譜圖  
B. 添加背景噪聲、改變語速/音調、頻域遮擋 (SpecAugment)  
C. 倒放  
D. 做成鬼畜視頻

**正確答案：B**  
**解析：** 提高模型對噪聲環境的魯棒性。

### 53. 喚醒詞檢測 (Keyword Spotting) (如 "Hey Siri") 特點是？
A. 高延遲  
B. 低功耗、實時響應、本地運行  
C. 雲端處理  
D. 大模型

**正確答案：B**  
**解析：** Always-on 的邊緣端小模型。

### 54. 遠場語音識別 (Far-field ASR) 的難點在於？
A. 聲音太大  
B. 混響 (Reverberation) 和 迴聲 (Echo) 干擾嚴重  
C. 麥克風太貴  
D. 沒有網

**正確答案：B**  
**解析：** 智能音箱常見場景，需結合麥克風陣列技術。

### 55. 語音識別中的 WER 代表？
A. Word Error Rate (詞錯誤率)  
B. World Exchange Rate  
C. Weighted Error Ratio  
D. Windows Error Report

**正確答案：A**  
**解析：** 越低越好，是 ASR 的核心評估指標。

### 56. 頻譜圖 (Spectrogram) 是將聲音信號轉換為？
A. 文本  
B. 圖像 (時間-頻率-強度)  
C. 數字列表  
D. 視頻

**正確答案：B**  
**解析：** 讓語音問題可以利用 CNN 等圖像技術處理。

### 57. 語言模型中的 N-gram 用於語音識別時，可以？
A. 消除噪聲  
B. 根據上文預測下一個詞，從而糾正發音相似但語法錯誤的詞  
C. 識別方言  
D. 翻譯

**正確答案：B**  
**解析：** "Recognize speech" vs "Wreck a nice beach"。

### 58. 半監督學習在語音識別中的應用？
A. 生成偽標籤 (Pseudo-labeling) 利用未標註語音數據  
B. 人工標註所有數據  
C. 不使用未標註數據  
D. 只用文本訓練

**正確答案：A**  
**解析：** 各種 Self-training 技術。

### 59. 混合專家模型 (MoE) 在語音領域可以用於？
A. 只處理英語  
B. 針對不同語言或口音激活不同的專家網絡  
C. 降低準確率  
D. 增加延遲

**正確答案：B**  
**解析：** 提升多語言模型的容量和效果。

### 60. 語音合成 (TTS) 與語音識別的關係是？
A. 互逆過程  
B. 沒關係  
C. 相同的過程  
D. TTS 是 ASR 的子集

**正確答案：A**  
**解析：** 常用於 ASR 的數據增強 (回譯)。

---

## 第四部分：推薦系統與決策 (20題)

### 61. 協同過濾 (Collaborative Filtering) 的核心假設是？
A. 相似的用戶會喜歡相似的東西  
B. 內容相似的商品會被同一個人喜歡  
C. 價格越低越好  
D. 隨機推薦

**正確答案：A**  
**解析：** 利用群體智慧挖掘潛在興趣。

### 62. 基於內容的推薦 (Content-based Recommendation) 依賴於？
A. 用戶的評分歷史  
B. 物品的屬性特徵 (如 電影的導演、類型) 和用戶的興趣畫像  
C. 其他用戶的行為  
D. 隨機

**正確答案：B**  
**解析：** "因為你看了成龍的電影，所以推薦另一部成龍的電影"。

### 63. 推薦系統中的「冷啟動 (Cold Start)」問題是指？
A. 伺服器太冷  
B. 系統剛啟動時沒有用戶數據，或者新用戶/新物品缺乏行為記錄，導致無法精準推薦  
C. 計算速度慢  
D. 用戶不喜歡推薦

**正確答案：B**  
**解析：** 用戶-物品矩陣是空的。

### 64. 混合推薦 (Hybrid Recommendation) 的目的是？
A. 增加複雜度  
B. 結合多種推薦算法 (如 CF + 內容) 的優點，克服單一算法的缺陷 (如 冷啟動、稀疏性)  
C. 節省計算資源  
D. 隨便混合

**正確答案：B**  
**解析：** 業界主流方案。

### 65. 矩陣分解 (Matrix Factorization) 常用於？
A. 內容推薦  
B. 協同過濾，預測用戶對未交互物品的評分 (填補矩陣)  
C. 分類  
D. 聚類

**正確答案：B**  
**解析：** 將 User-Item 矩陣分解為 User 隱向量和 Item 隱向量的內積。

### 66. A/B 測試在推薦系統中的作用？
A. 檢查代碼錯誤  
B. 在線上流量中對比不同推薦算法對核心指標 (如 CTR, GMV) 的影響  
C. 離線評估  
D. 用戶訪談

**正確答案：B**  
**解析：** 這是評估推薦效果的最終標準。

### 67. 點擊率 (CTR) 預測是推薦系統中的？
A. 生成任務  
B. 分類/回歸任務 (預測用戶點擊的機率)  
C. 聚類任務  
D. 翻譯任務

**正確答案：B**  
**解析：** 典型的鑑別式 AI 應用。

### 68. 召回 (Recall) 階段在推薦系統架構中的作用是？
A. 精確排序  
B. 從海量候選集中快速篩選出一小部分用戶可能感興趣的物品  
C. 過濾敏感內容  
D. 生成解釋

**正確答案：B**  
**解析：** 粗排/海選，關注覆蓋率和速度。

### 69. 排序 (Ranking) 階段在推薦系統中的作用是？
A. 初步篩選  
B. 對召回的物品進行精細打分和排序，決定最終展示順序  
C. 刪除物品  
D. 歸檔

**正確答案：B**  
**解析：** 精排，使用複雜模型和大量特徵。

### 70. 隱式反饋 (Implicit Feedback) 數據包括？
A. 用戶的 1-5 星評分  
B. 點擊、瀏覽時長、購買、添加到購物車  
C. 用戶評論  
D. 問卷調查

**正確答案：B**  
**解析：** 量大但有噪聲，比顯式評分更常見。

### 71. 探索與利用 (Exploration vs Exploitation) 的權衡是指？
A. 推薦已知的用戶喜歡的 (利用) vs 推薦新鮮的未知的 (探索) 以獲取更多反饋  
B. 開發新系統 vs 維護舊系統  
C. 挖掘數據 vs 刪除數據  
D. 買機器 vs 租機器

**正確答案：A**  
**解析：** 避免信息繭房，保持長期的用戶留存。

### 72. 深度學習推薦模型 (如 Wide & Deep, YouTube DNN) 的優勢？
A. 可解釋性強  
B. 能處理海量特徵，自動進行特徵交叉，擬合能力強  
C. 計算量小  
D. 不需要數據

**正確答案：B**  
**解析：** 顯著提升了 CTR 預測準度。

### 73. 用戶畫像 (User Profile) 是？
A. 用戶的照片  
B. 用戶特徵的標籤化集合 (如 年齡、興趣、歷史行為)  
C. 用戶名  
D. 密碼

**正確答案：B**  
**解析：** 個人化推薦的基礎。

### 74. 上下文感知推薦 (Context-aware Recommendation) 考慮了？
A. 只有用戶和物品  
B. 時間、地點、設備、當前活動等情境信息  
C. 只有時間  
D. 只有地點

**正確答案：B**  
**解析：** "週五晚上在家" 和 "週一早上在通勤" 推薦的東西應該不同。

### 75. 協同過濾的主要缺點之一是「稀疏性 (Sparsity)」，指？
A. 用戶太少  
B. 大多數用戶只評價了極少部分的物品，導致矩陣大部分是空的  
C. 物品太少  
D. 系統太慢

**正確答案：B**  
**解析：** 難以找到相似用戶。

### 76. 重排 (Re-ranking) 階段通常為了？
A. 提高 CTR  
B. 業務規則控制 (如 多樣性打散、去重、強插廣告)  
C. 減少計算  
D. 隨機排序

**正確答案：B**  
**解析：** 兼顧商業目標和用戶體驗。

### 77. 負採樣 (Negative Sampling) 在推薦模型訓練中用於？
A. 尋找差評  
B. 從未觀測數據中選取一部分作為負樣本 (非興趣物品)，平衡正負樣本比例  
C. 刪除負樣本  
D. 增加正樣本

**正確答案：B**  
**解析：** 隱式反饋通常只有正樣本，需要人工構造負樣本。

### 78. 序列推薦 (Sequential Recommendation) 重視？
A. 用戶行為的時間順序 (如 買了手機 -> 買手機殼)  
B. 用戶的性別  
C. 物品的價格  
D. 全局熱度

**正確答案：A**  
**解析：** 用 RNN/Transformer 捕捉動態興趣演變。

### 79. 知識圖譜 (Knowledge Graph) 在推薦中的作用？
A. 存儲圖片  
B. 引入外部語義信息，增強物品之間的關聯性 (可解釋性推薦)  
C. 繪圖  
D. 加密

**正確答案：B**  
**解析：** 解決稀疏性，提供推薦理由。

### 80. 多任務學習 (Multi-task Learning) 在推薦系統中常用於？
A. 同時優化多個目標 (如 點擊率、轉化率、停留時長)  
B. 只優化一個目標  
C. 一個模型學多種語言  
D. 用戶分類

**正確答案：A**  
**解析：** ESSM, MMOE 等模型，解決樣板選擇偏差並提升綜合收益。

---

## 答案總覽

| 題號 | 答案 | 題號 | 答案 | 題號 | 答案 | 題號 | 答案 |
|------|------|------|------|------|------|------|------|
| 1 | B | 21 | B | 41 | A | 61 | A |
| 2 | B | 22 | B | 42 | B | 62 | B |
| 3 | B | 23 | A | 43 | B | 63 | B |
| 4 | B | 24 | B | 44 | B | 64 | B |
| 5 | B | 25 | B | 45 | B | 65 | B |
| 6 | B | 26 | A | 46 | B | 66 | B |
| 7 | B | 27 | B | 47 | B | 67 | B |
| 8 | B | 28 | B | 48 | B | 68 | B |
| 9 | B | 29 | B | 49 | B | 69 | B |
| 10 | B | 30 | B | 50 | B | 70 | B |
| 11 | B | 31 | B | 51 | B | 71 | A |
| 12 | B | 32 | B | 52 | B | 72 | B |
| 13 | B | 33 | B | 53 | B | 73 | B |
| 14 | B | 34 | B | 54 | B | 74 | B |
| 15 | B | 35 | C | 55 | A | 75 | B |
| 16 | B | 36 | B | 56 | B | 76 | B |
| 17 | B | 37 | D | 57 | B | 77 | B |
| 18 | B | 38 | B | 58 | A | 78 | A |
| 19 | B | 39 | B | 59 | B | 79 | B |
| 20 | A | 40 | A | 60 | A | 80 | A |
