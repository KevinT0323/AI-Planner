# L21101 自然語言處理技術與應用 - 詳細說明

## 一、核心概念與定義

### 1.1 自然語言處理(NLP)定義
自然語言處理(Natural Language Processing, NLP)是人工智慧的一個重要分支，旨在讓電腦能夠理解、處理和生成人類自然語言（文字或語音）的技術。

### 1.2 NLP的發展歷程
- **1950年代**：機器翻譯的早期嘗試
- **1980-1990年代**：基於規則的方法
- **2000年代**：統計機器學習方法
- **2010年代**：深度學習革命
- **2017年至今**：Transformer架構與大型語言模型(LLM)時代

### 1.3 NLP的主要任務分類
1. **理解任務**：文本分類、情感分析、命名實體識別、語義分析
2. **生成任務**：文本生成、機器翻譯、摘要生成、對話生成
3. **轉換任務**：語音轉文字、文字轉語音、語言轉換

---

## 二、核心技術詳解

### 2.1 文本預處理技術

#### 2.1.1 分詞(Tokenization)
- **定義**：將連續的文本分割成單獨的詞彙單元
- **中文分詞挑戰**：
  - 無明顯詞邊界
  - 歧義問題（如「南京市長江大橋」）
  - 新詞識別
- **分詞方法**：
  - 基於規則：最大匹配法、最小匹配法
  - 基於統計：N-gram模型、隱馬爾可夫模型(HMM)
  - 基於深度學習：BiLSTM-CRF、BERT-based

#### 2.1.2 停用詞移除(Stop Word Removal)
- **目的**：移除對語義貢獻較小的常用詞
- **常見停用詞**：的、了、在、是、我、有、和、就等
- **注意事項**：某些任務中停用詞可能包含重要信息

#### 2.1.3 詞幹提取(Stemming)與詞形還原(Lemmatization)
- **詞幹提取**：將詞還原到詞根形式（如running → run）
- **詞形還原**：考慮詞的語法角色，還原到字典形式（更準確）
- **中文特點**：中文沒有詞形變化，此技術主要用於英文處理

#### 2.1.4 正規化處理
- 大小寫統一
- 數字標準化
- 標點符號處理
- 特殊字符清理

### 2.2 特徵提取技術

#### 2.2.1 Bag of Words (BoW)
- **原理**：將文本表示為詞彙的集合，忽略詞序
- **優點**：簡單直觀
- **缺點**：忽略語序和語義關係
- **應用**：文本分類、情感分析

#### 2.2.2 TF-IDF (Term Frequency-Inverse Document Frequency)
- **TF (詞頻)**：詞在文檔中出現的頻率
- **IDF (逆文檔頻率)**：衡量詞的稀有程度
- **公式**：TF-IDF = TF × IDF
- **應用**：文本檢索、關鍵詞提取

#### 2.2.3 Word Embeddings (詞嵌入)
- **Word2Vec**：
  - Skip-gram模型：用中心詞預測上下文
  - CBOW模型：用上下文預測中心詞
  - 優點：捕捉語義相似性
- **GloVe (Global Vectors)**：
  - 結合全局統計信息
  - 更好的語義表示
- **FastText**：
  - 考慮子詞(subword)信息
  - 對未登錄詞(OOV)處理更好

#### 2.2.4 上下文嵌入(Contextual Embeddings)
- **ELMo (Embeddings from Language Models)**：
  - 雙向LSTM
  - 詞的表示依賴上下文
- **BERT (Bidirectional Encoder Representations from Transformers)**：
  - 雙向Transformer編碼器
  - 掩碼語言模型(MLM)預訓練
  - 下一句預測(NSP)任務
- **GPT系列**：
  - 單向Transformer解碼器
  - 自回歸生成模型

### 2.3 深度學習模型架構

#### 2.3.1 RNN (Recurrent Neural Network)
- **原理**：使用循環連接處理序列數據
- **優點**：能處理變長序列
- **缺點**：梯度消失/爆炸問題
- **應用**：語言建模、序列標註

#### 2.3.2 LSTM (Long Short-Term Memory)
- **改進**：解決RNN的長期依賴問題
- **核心機制**：
  - 遺忘門(Forget Gate)
  - 輸入門(Input Gate)
  - 輸出門(Output Gate)
  - 細胞狀態(Cell State)
- **應用**：機器翻譯、文本生成

#### 2.3.3 GRU (Gated Recurrent Unit)
- **特點**：LSTM的簡化版本
- **優勢**：參數更少，訓練更快
- **應用**：與LSTM類似，但計算效率更高

#### 2.3.4 Transformer架構
- **核心創新**：
  - 自注意力機制(Self-Attention)
  - 位置編碼(Positional Encoding)
  - 多頭注意力(Multi-Head Attention)
- **優勢**：
  - 並行計算效率高
  - 捕捉長距離依賴
  - 可擴展性強
- **架構組成**：
  - 編碼器(Encoder)
  - 解碼器(Decoder)
  - 前饋神經網路(Feed-Forward Network)

#### 2.3.5 預訓練模型
- **BERT系列**：
  - BERT-base (110M參數)
  - BERT-large (340M參數)
  - RoBERTa (優化的BERT)
  - ALBERT (參數共享的BERT)
- **GPT系列**：
  - GPT-1 (117M參數)
  - GPT-2 (1.5B參數)
  - GPT-3 (175B參數)
  - GPT-4 (多模態模型)
- **T5 (Text-to-Text Transfer Transformer)**：
  - 統一的文本到文本框架
  - 所有NLP任務都轉換為生成任務

---

## 三、主要應用領域

### 3.1 文本分類
- **應用場景**：
  - 垃圾郵件檢測
  - 新聞分類
  - 情感分析
  - 主題分類
- **技術方法**：
  - 傳統方法：SVM、樸素貝葉斯
  - 深度學習：CNN、LSTM、BERT

### 3.2 命名實體識別(NER)
- **任務目標**：識別文本中的實體（人名、地名、組織名等）
- **方法**：
  - 基於規則
  - 基於統計：CRF、HMM
  - 基於深度學習：BiLSTM-CRF、BERT

### 3.3 情感分析
- **任務類型**：
  - 二分類：正面/負面
  - 多分類：正面/中性/負面
  - 細粒度：多維度情感分析
- **應用**：產品評論分析、社交媒體監控

### 3.4 機器翻譯
- **發展歷程**：
  - 基於規則的翻譯
  - 統計機器翻譯(SMT)
  - 神經機器翻譯(NMT)
- **現代方法**：
  - Seq2Seq模型
  - Transformer-based模型
  - 多語言模型

### 3.5 問答系統
- **類型**：
  - 檢索式問答：從知識庫中檢索答案
  - 生成式問答：生成答案
  - 閱讀理解：從給定文本中提取答案
- **技術**：
  - BERT for QA
  - GPT系列
  - RAG (Retrieval-Augmented Generation)

### 3.6 文本摘要
- **類型**：
  - 抽取式摘要：從原文中提取句子
  - 生成式摘要：生成新的摘要文本
- **方法**：
  - 傳統：TextRank、LSA
  - 深度學習：Seq2Seq、Transformer

### 3.7 對話系統
- **類型**：
  - 任務導向型：完成特定任務
  - 開放域對話：自由對話
- **技術**：
  - 檢索式：從預定義回復中選擇
  - 生成式：生成回復
  - 混合式：結合檢索和生成

---

## 四、實務應用案例

### 4.1 智能客服系統
- **功能**：
  - 自動回答常見問題
  - 意圖識別
  - 情感分析
  - 多輪對話管理
- **技術架構**：
  - 意圖分類模型
  - 實體識別模型
  - 對話管理系統
  - 知識庫檢索

### 4.2 智能文檔處理
- **應用**：
  - 文檔自動分類
  - 關鍵信息提取
  - 合同分析
  - 法律文檔審查
- **技術**：
  - 文檔理解模型
  - 表格識別
  - 實體關係抽取

### 4.3 內容審核
- **功能**：
  - 有害內容檢測
  - 垃圾信息過濾
  - 虛假信息識別
- **挑戰**：
  - 對抗性樣本
  - 語境理解
  - 多語言支持

---

## 五、技術挑戰與解決方案

### 5.1 常見挑戰
1. **歧義問題**：一詞多義、語法歧義
2. **語言多樣性**：不同語言、方言、俚語
3. **上下文依賴**：需要理解長距離依賴
4. **數據稀缺**：低資源語言數據不足
5. **偏見問題**：模型可能學習到數據中的偏見

### 5.2 解決方案
- **預訓練模型**：利用大規模無標註數據
- **遷移學習**：從高資源語言遷移到低資源語言
- **數據增強**：生成更多訓練數據
- **多任務學習**：同時學習多個相關任務
- **可解釋性研究**：理解模型決策過程

---

## 六、評估指標

### 6.1 分類任務指標
- **準確率(Accuracy)**
- **精確率(Precision)**
- **召回率(Recall)**
- **F1-Score**
- **混淆矩陣**

### 6.2 生成任務指標
- **BLEU**：機器翻譯質量評估
- **ROUGE**：摘要質量評估
- **METEOR**：考慮同義詞的評估
- **BERTScore**：基於語義相似度的評估

### 6.3 其他指標
- **困惑度(Perplexity)**：語言模型評估
- **語義相似度**：詞向量質量評估

---

## 七、最新發展趨勢(2024-2025)

### 7.1 大型語言模型(LLM)
- **趨勢**：模型規模持續增大
- **多模態融合**：文本+圖像+語音
- **指令微調**：提高模型可控性
- **對齊技術**：RLHF (Reinforcement Learning from Human Feedback)

### 7.2 效率優化
- **模型壓縮**：量化、剪枝、蒸餾
- **高效架構**：Flash Attention、LoRA
- **邊緣部署**：移動端、IoT設備

### 7.3 可解釋性與安全性
- **可解釋AI**：理解模型決策
- **對抗性防禦**：對抗攻擊檢測
- **隱私保護**：聯邦學習、差分隱私

---

## 八、考試重點提醒

### 8.1 必記概念
- NLP的定義與主要任務
- 各種特徵提取方法的優缺點
- Transformer架構的核心機制
- 預訓練模型的發展歷程

### 8.2 技術比較
- RNN vs LSTM vs GRU
- BERT vs GPT
- 抽取式摘要 vs 生成式摘要
- 檢索式問答 vs 生成式問答

### 8.3 應用場景判斷
- 能夠根據應用場景選擇合適的NLP技術
- 理解不同任務的評估指標
- 掌握常見的預處理流程

### 8.4 實務考量
- 數據質量對模型性能的影響
- 計算資源與模型選擇的權衡
- 多語言處理的挑戰

---

## 九、參考資源

### 9.1 經典論文
- Attention Is All You Need (Transformer)
- BERT: Pre-training of Deep Bidirectional Transformers
- GPT系列論文

### 9.2 實用工具
- **Python庫**：NLTK、spaCy、Transformers (Hugging Face)
- **框架**：PyTorch、TensorFlow
- **預訓練模型**：Hugging Face Model Hub

### 9.3 學習資源
- 線上課程：Coursera、edX
- 技術博客：Towards Data Science、Papers with Code
- 開源項目：GitHub上的NLP項目

---

**考試日期：2026/05/23**
